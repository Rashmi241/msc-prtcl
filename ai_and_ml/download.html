
    
<!DOCTYPE html>




<html lang="en" >
<head >
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <!-- Mobile properties -->
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

  
    <!-- Stylesheets -->
    <link rel="stylesheet" href="/pmc/static/CACHE/css/output.aa42804411e1.css" type="text/css">
  
  <link rel="stylesheet" href="/pmc/static/CACHE/css/output.6c8346f5860b.css" type="text/css"><link rel="stylesheet" href="/pmc/static/CACHE/css/output.3b12438b8d66.css" type="text/css"><link rel="stylesheet" href="/pmc/static/CACHE/css/output.3766d7ad0d2d.css" type="text/css"><link rel="stylesheet" href="/pmc/static/CACHE/css/output.e3c3c2c84eb3.css" type="text/css">

  
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"/>
  
<link type="text/css" href="/pmc/static/bundles/base/base.6539a0a78536cfdc1fa6.css" rel="stylesheet" />


    <link rel="stylesheet" href="/corehtml/pmc/css/fonts/stix/stixfonts.css" type="text/css" /><link rel="stylesheet" href="/corehtml/pmc/css/3.18/pmcrefs1.min.css" type="text/css" /><link rel="stylesheet" href="/corehtml/pmc/css/pmc2020_1.1/ncbi_web.min.css?_=a" type="text/css" /><style type="text/css">.pmc-wm {background:transparent repeat-y top left;background-image:url(/corehtml/pmc/pmcgifs/wm-entropy.gif);background-size: auto, contain}</style><style type="text/css">.print-view{display:block}</style>

    <link type="text/css" href="/pmc/static/bundles/article/article.e622fd8870cb011febf1.css" rel="stylesheet" />
    <link type="text/css" href="/pmc/static/django_pmc_cite_box/lib/cite-box.css" rel="stylesheet" />


    <title>Long-Range Dependence Involutional Network for Logo Detection - PMC</title>

  
  <!-- Favicons -->
  <link rel="shortcut icon" type="image/ico" href="https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon.ico" />
  <link rel="icon" type="image/png" href="https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon.png" />

  <!-- 192x192, as recommended for Android
  http://updates.html5rocks.com/2014/11/Support-for-theme-color-in-Chrome-39-for-Android
  -->
  <link rel="icon" type="image/png" sizes="192x192" href="https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon-192.png" />

  <!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
  <link rel="apple-touch-icon-precomposed" href="https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon-57.png">
  <!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
  <link rel="apple-touch-icon-precomposed" sizes="72x72" href="https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon-72.png">
  <!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
  <link rel="apple-touch-icon-precomposed" sizes="114x114" href="https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon-114.png">
  <!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon-144.png">



    
        <!-- Logging params: Pinger defaults -->

<meta name="ncbi_app" content="pmc-frontend" />

<meta name="ncbi_db" content="pmc" />

<meta name="ncbi_phid" content="322C53D3A2057675000057973448E173.1.m_2" />


        <!-- Logging params: Pinger custom -->

<meta name="ncbi_pdid" content="article" />

<meta name="ncbi_op" content="retrieved" />

<meta name="ncbi_app_version" content="1.5.0.post1+55721b3" />

<meta name="ncbi_domain" content="entropy" />

<meta name="ncbi_type" content="fulltext" />

<meta name="ncbi_pcid" content="/articles/PMC9857861/" />


    


        <script>
            
            var useOfficialGovtHeader = true;
        </script>


    <meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE" /><link rel="canonical" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9857861/" /><link rel="schema.DC" href="http://purl.org/DC/elements/1.0/" /><meta name="citation_journal_title" content="Entropy" /><meta name="citation_title" content="Long-Range Dependence Involutional Network for Logo Detection" /><meta name="citation_author" content="Xingzhuo Li" /><meta name="citation_author" content="Sujuan Hou" /><meta name="citation_author" content="Baisong Zhang" /><meta name="citation_author" content="Jing Wang" /><meta name="citation_author" content="Weikuan Jia" /><meta name="citation_author" content="Yuanjie Zheng" /><meta name="citation_publication_date" content="2023/01" /><meta name="citation_issue" content="1" /><meta name="citation_volume" content="25" /><meta name="citation_doi" content="10.3390/e25010174" /><meta name="citation_abstract_html_url" content="/pmc/articles/PMC9857861/?report=abstract" /><meta name="citation_fulltext_html_url" content="/pmc/articles/PMC9857861/" /><meta name="citation_pmid" content="36673315" /><meta name="DC.Title" content="Long-Range Dependence Involutional Network for Logo Detection" /><meta name="DC.Type" content="Text" /><meta name="DC.Publisher" content="Multidisciplinary Digital Publishing Institute  (MDPI)" /><meta name="DC.Contributor" content="Xingzhuo Li" /><meta name="DC.Contributor" content="Sujuan Hou" /><meta name="DC.Contributor" content="Baisong Zhang" /><meta name="DC.Contributor" content="Jing Wang" /><meta name="DC.Contributor" content="Weikuan Jia" /><meta name="DC.Contributor" content="Yuanjie Zheng" /><meta name="DC.Date" content="2023 Jan" /><meta name="DC.Identifier" content="10.3390/e25010174" /><meta name="DC.Language" content="en" /><meta property="og:title" content="Long-Range Dependence Involutional Network for Logo Detection" /><meta property="og:type" content="article" /><meta property="og:description" content="Logo detection is one of the crucial branches in computer vision due to various real-world applications, such as automatic logo detection and recognition, intelligent transportation, and trademark infringement detection. Compared with traditional handcrafted-feature-based ..." /><meta property="og:url" content="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9857861/" /><meta property="og:site_name" content="PubMed Central (PMC)" /><meta property="og:image" content="https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0" /><meta name="twitter:card" content="summary_large_image" /><meta name="twitter:site" content="@ncbi" /><meta name="ncbi_feature" content="associated_data" />


</head>
<body >

   
    
    


    
        <section class="pmc-alerts">

</section>
    

    
        <button
          class="back-to-top back-to-top--bottom"
          data-ga-category="pagination"
          data-ga-action="back_to_top">
          Back to Top
        </button>
    

    <a class="usa-skipnav" href="#main-content">Skip to main content</a>
    <!-- ========== BEGIN HEADER ========== -->
<section class="usa-banner" style="display: none;">
  <div class="usa-accordion">
    <header class="usa-banner-header">
      <div class="usa-grid usa-banner-inner">
        <img src="https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/favicons/favicon-57.png" alt="U.S. flag" />
        <p>An official website of the United States government</p>
        <button
          class="usa-accordion-button usa-banner-button"
          aria-expanded="false"
          aria-controls="gov-banner-top"
        >
          <span class="usa-banner-button-text">Here's how you know</span>
        </button>
      </div>
    </header>
    <div
      class="usa-banner-content usa-grid usa-accordion-content"
      id="gov-banner-top"
    >
      <div class="usa-banner-guidance-gov usa-width-one-half">
        <img
          class="usa-banner-icon usa-media_block-img"
          src="https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-dot-gov.svg"
          alt="Dot gov"
        />
        <div class="usa-media_block-body">
          <p>
            <strong>The .gov means it’s official.</strong>
            <br />
            Federal government websites often end in .gov or .mil. Before
            sharing sensitive information, make sure you’re on a federal
            government site.
          </p>
        </div>
      </div>
      <div class="usa-banner-guidance-ssl usa-width-one-half">
        <img
          class="usa-banner-icon usa-media_block-img"
          src="https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-https.svg"
          alt="Https"
        />
        <div class="usa-media_block-body">
          <p>
            <strong>The site is secure.</strong>
            <br />
            The <strong>https://</strong> ensures that you are connecting to the
            official website and that any information you provide is encrypted
            and transmitted securely.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<div class="usa-overlay"></div>
<header class="ncbi-header" role="banner" data-section="Header">

	<div class="usa-grid">
		<div class="usa-width-one-whole">

            <div class="ncbi-header__logo">
                <a href="https://www.ncbi.nlm.nih.gov/" class="logo" aria-label="NCBI Logo" data-ga-action="click_image" data-ga-label="NIH NLM Logo">
                  <img src="https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/logos/AgencyLogo.svg" alt="NIH NLM Logo" />
                </a>
            </div>

			<div class="ncbi-header__account">
				<a id="account_login" href="https://account.ncbi.nlm.nih.gov" class="usa-button header-button" style="display:none" data-ga-action="open_menu" data-ga-label="account_menu">Log in</a>
				<button id="account_info" class="header-button" style="display:none"
						aria-controls="account_popup">
					<span class="fa fa-user" aria-hidden="true"></span>
					<span class="username desktop-only" aria-hidden="true" id="uname_short"></span>
					<span class="sr-only">Show account info</span>
				</button>
			</div>

			<div class="ncbi-popup-anchor">
				<div class="ncbi-popup account-popup" id="account_popup" aria-hidden="true">
					<div class="ncbi-popup-head">
						<button class="ncbi-close-button" data-ga-action="close_menu" data-ga-label="account_menu"><span class="fa fa-times"></span><span class="usa-sr-only">Close</span></button>
						<h4>Account</h4>
					</div>
					<div class="account-user-info">
						Logged in as:<br/>
						<b><span class="username" id="uname_long">username</span></b>
					</div>
					<div class="account-links">
						<ul class="usa-unstyled-list">
							<li><a id="account_myncbi" href="/myncbi/" class="set-base-url" data-ga-action="click_menu_item" data-ga-label="account_myncbi">Dashboard</a></li>
							<li><a id="account_pubs" href="/myncbi/collections/bibliography/" class="set-base-url" data-ga-action="click_menu_item" data-ga-label="account_pubs">Publications</a></li>
							<li><a id="account_settings" href="/account/settings/" class="set-base-url" data-ga-action="click_menu_item" data-ga-label="account_settings">Account settings</a></li>
							<li><a id="account_logout" href="/account/signout/" class="set-base-url" data-ga-action="click_menu_item" data-ga-label="account_logout">Log out</a></li>
						</ul>
					</div>
				</div>
			</div>

		</div>
	</div>
</header>
<div role="navigation" aria-label="access keys">
<a id="nws_header_accesskey_0" href="https://www.ncbi.nlm.nih.gov/guide/browsers/#ncbi_accesskeys" class="usa-sr-only" accesskey="0" tabindex="-1">Access keys</a>
<a id="nws_header_accesskey_1" href="https://www.ncbi.nlm.nih.gov" class="usa-sr-only" accesskey="1" tabindex="-1">NCBI Homepage</a>
<a id="nws_header_accesskey_2" href="/myncbi/" class="set-base-url usa-sr-only" accesskey="2" tabindex="-1">MyNCBI Homepage</a>
<a id="nws_header_accesskey_3" href="#maincontent" class="usa-sr-only" accesskey="3" tabindex="-1">Main Content</a>
<a id="nws_header_accesskey_4" href="#" class="usa-sr-only" accesskey="4" tabindex="-1">Main Navigation</a>
</div>
<section data-section="Alerts">
	<div class="ncbi-alerts-placeholder"></div>
</section>
<!-- ========== END HEADER ========== -->

    
    
        
        <header class="pmc-header usa-header-extended" role="banner">
    <div class="pmc-header__bar">
        <div class="pmc-header__control usa-accordion">
            
                <button class="usa-menu-btn pmc-header--button pmc-header--left">
                    <i class="fa fa-ellipsis-v" aria-hidden="true"></i>
                </button>
            

            <div class="usa-logo pmc-header__logo pmc-header--stretch
                
               " id="extended-mega-logo">
                <div class="usa-logo-text">
                    <a href="/pmc/" title="Home" aria-label="Home" ></a>
                </div>
            </div>
            <button class="usa-accordion-button pmc-header--right pmc-header--button pmc-header__search--control pmc-header--right" aria-expanded="false" aria-controls="a1">
                <i class="fa fa-search" aria-hidden="true"></i>
                <i class="fa fa-times" aria-hidden="true"></i>
            </button>
        </div>
        <div class="pmc-header__search usa-accordion-content" id="a1">
            <div role="search"  class="pmc-header--stretch"  >
    <form class="usa-search " autocomplete="off">
        <div>
              <label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
              <span class="clearable">
                <input
                  required="required"
                  autocomplete-url="/pmc/autocomplete/pmc/"
                  placeholder="Search PMC Full-Text Archive"
                  id="pmc-search" type="search" name="term"/>
                  <i class="clear-btn" ></i>
              </span>
              <button type="submit" formaction="/pmc/">
                <span class="usa-search-submit-text">Search in PMC</span>
              </button>
        </div>
    </form>
</div>


            <ul class="usa-unstyled-list usa-nav-secondary-links pmc-header--offset-two">
                    <li>
                        <a href="https://www.ncbi.nlm.nih.gov/pmc/advanced" data-ga-action="featured_link" data-ga-label="advanced_search">
                            Advanced Search
                        </a>
                    </li>
                    <li>
                        <a  href="/pmc/about/userguide/" data-ga-action="featured_link"
                        data-ga-label="user guide">
                            User Guide
                        </a>
                    </li>
            </ul>
        </div>
    </div>
     <nav role="navigation" class="usa-nav ">
        <button class="usa-nav-close">
            <i class="fa fa-times" aria-hidden="true"></i>
        </button>
        <div class="usa-breadcrumb usa-breadcrumb--wrap usa-breadcrumb--hack">
             
    <ul class="usa-breadcrumb__list">
            
                <li class="usa-breadcrumb__list-item"
                    >
                    <a href="/pmc/journals/" class="navlink">Journal List</a>
                </li>
            
                <li class="usa-breadcrumb__list-item"
                    >
                    <a href="/pmc/?term=%22Entropy%20(Basel)%22[journal]" class="navlink">Entropy (Basel)</a>
                </li>
            
                <li class="usa-breadcrumb__list-item"
                     aria-current="page" >
                    PMC9857861
                </li>
            
    </ul>
 

        </div>
        
        
            <div class="pmc-sidebar pmc-sidebar-hack">
                

<div class="scroller">

    
        <section>
                <h6>Other Formats</h6>
                <ul class="pmc-sidebar__formats">
                  <li class="pubreader-link other_item"><a href="/pmc/articles/PMC9857861/?report=reader" class="sidefm-pmclink">PubReader</a></li>
<li class="pdf-link other_item"><a href="/pmc/articles/PMC9857861/pdf/entropy-25-00174.pdf" class="int-view">PDF (3.0M)</a></li>
                </ul>
        </section>
    
    <section>
        <h6>Actions</h6>
        <ul class="pmc-sidebar__actions">
            <li>
                <button role="button" class="citation-button citation-dialog-trigger ctxp"
                        aria-label="Open dialog with citation text in different styles" data-ga-category="save_share" data-ga-action="cite" data-ga-label="open"
                        data-all-citations-url="/pmc/resources/citations/9857861/"
                        data-citation-style="nlm"
                        data-download-format-link="/pmc/resources/citations/9857861/export/"
                >
                    <span class="button-label">Cite</span>
                </button>
            </li>
            <li>
                
                    

<div class="collections-button-container" data-article-id="9857861" data-article-db="pmc">
  <button class="collections-button collections-dialog-trigger"
          aria-label="Save article in MyNCBI collections."
          data-ga-category="collections_button"
          data-ga-action="click"
          data-ga-label="collections_button"
          data-collections-open-dialog-enabled="false"
          data-collections-open-dialog-url="https://www.ncbi.nlm.nih.gov/account?back_url=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC9857861%2F%23open-collections-dialog"
          data-in-collections="false">
      <span class="button-label">Collections</span>
  </button>
  <div class="overlay" role="dialog">
  <div id="collections-action-dialog"
       class="dialog collections-dialog"
       aria-hidden="true">
    <div class="title">Add to Collections</div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/pmc/list-existing-collections/"
      data-add-to-existing-collection-url="/pmc/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/pmc/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

  <input type="hidden" name="csrfmiddlewaretoken" value="Q8UnHfwsxk4jqadNUceLBKWuZuqM4zGN7iTgTJMbt0CIdNYca6Gg2m3tb87thdUJ">

  

  <div class="choice-group" role="radiogroup">
    <ul class="radio-group-items">
      <li>
        <input type="radio"
               id="collections-action-dialog-new-header "
               class="collections-new"
               name="collections"
               value="new"
               data-ga-category="collections_button"
               data-ga-action="click"
               data-ga-label="collections_radio_new">
        <label for="collections-action-dialog-new-header ">Create a new collection</label>
      </li>
      <li>
        <input type="radio"
               id="collections-action-dialog-existing-header "
               class="collections-existing"
               name="collections"
               value="existing"
               checked="true"
               data-ga-category="collections_button"
               data-ga-action="click"
               data-ga-label="collections_radio_existing">
        <label for="collections-action-dialog-existing-header ">Add to an existing collection</label>
      </li>
    </ul>
  </div>

  <div class="controls-wrapper">
    <div class="action-panel-control-wrap new-collections-controls">
      <label for="collections-action-dialog-add-to-new" class="action-panel-label required-field-asterisk">
        Name your collection:
      </label>
      <input
        type="text"
        name="add-to-new-collection"
        id="collections-action-dialog-add-to-new"
        class="collections-action-add-to-new"
        pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
        maxlength=""
        data-ga-category="collections_button"
        data-ga-action="create_collection"
        data-ga-label="non_favorties_collection">
      <div class="collections-new-name-too-long usa-input-error-message selection-validation-message">
        Name must be less than  characters
      </div>
    </div>
    <div class="action-panel-control-wrap existing-collections-controls">
      <label for="collections-action-dialog-add-to-existing" class="action-panel-label">
        Choose a collection:
      </label>
      <select id="collections-action-dialog-add-to-existing"
              class="action-panel-selector collections-action-add-to-existing"
              data-ga-category="collections_button"
              data-ga-action="select_collection"
              data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
      </select>
      <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
        Unable to load your collection due to an error<br>
        <a href="#">Please try again</a>
      </div>
    </div>
  </div>

  <div class="action-panel-actions">
    <button class="action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
      Add
    </button>
    <button class="action-panel-cancel"
            aria-label="Close 'Add to Collections' panel"
            ref="linksrc=close_collections_panel"
            aria-controls="collections-action-panel"
            aria-expanded="false"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="cancel">
      Cancel
    </button>
  </div>
</form>
    </div>
  </div>
</div>
</div>
                
            </li>

        </ul>
    </section>
    
        <section class="social-sharing">
            <h6>Share</h6>
            <ul class="pmc-sidebar__share">
                <li><a class="fa-stack fa-lg" target="_blank" rel="noopener noreferrer" role="button" href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC9857861%2F&amp;text=Long-Range%20Dependence%20Involutional%20Network%20for%20Logo%20Detection" alt="Share on Twitter"><i class="fa fa-twitter fa-stack-1x">&#160;</i></a></li> 
<li><a class="fa-stack fa-lg" target="_blank" rel="noopener noreferrer" role="button" href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC9857861%2F" alt="Share on Facebook"><i class="fa fa-facebook fa-stack-1x">&#160;</i></a></li>
                <li>
                    <div class="share-permalink">
                        <button class="trigger"  alt="Show article permalink" aria-expanded="false" aria-haspopup="true">
                            <i class="fa-stack fa-lg" >
                                <i class="fa fa-link fa-stack-1x">&nbsp;</i>
                            </i>
                        </button>
                        <div class="dropdown dropdown-container" hidden>
                              <div class="title">
                                Permalink
                              </div>
                              <div class="content">
                                  <input type="text" class="permalink-text" value="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9857861/" aria-label="Article permalink"><button class="permalink-copy-button usa-button-primary" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                                      <span class="button-title">Copy</span>
                                  </button>
                              </div>
                        </div>
                    </div>
                </li>
            </ul>
        </section>
    
    <section>
        <h6>RESOURCES</h6>
        <ul class="pmc-sidebar__resources">
        
            <li>
                <div class="usa-accordion">
                    <button
                            data-ga-category="resources_accordion"
                            data-ga-action="open_similar_articles"
                            data-ga-label="/pmc/articles/PMC9857861/"
                            class="usa-accordion-button"
                            aria-controls="similar-articles-accordion-header"
                            aria-expanded="false"
                            data-action-open="open_similar_articles"
                            data-action-close="close_similar_articles"
                    >
                        Similar articles
                    </button>
                    <div
                            
                                data-source-url="/pmc/resources/similar-article-links/36673315/"
                            

                         class="usa-accordion-content pmc-sidebar__resources--citations" id="similar-articles-accordion-header" aria-hidden="true">
                        
                    </div>
                </div>
            </li>
            <li>
                <div class="usa-accordion">
                    <button
                            data-ga-category="resources_accordion"
                            data-ga-action="open_cited_by"
                            data-ga-label="/pmc/articles/PMC9857861/"
                            class="usa-accordion-button"
                            aria-controls="cited-by-accordion-header"
                            aria-expanded="false"
                            data-action-open="open_cited_by"
                            data-action-close="close_cited_by"
                    >
                        Cited by other articles
                    </button>
                    <div
                            
                                data-source-url="/pmc/resources/cited-by-links/36673315/"
                            
                            class="usa-accordion-content pmc-sidebar__resources--citations"
                            id="cited-by-accordion-header"
                            aria-hidden="true"
                    >
                        
                    </div>
                </div>
            </li>
            <li>
                <div class="usa-accordion">
                    <button
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/pmc/articles/PMC9857861/"
                            class="usa-accordion-button"
                            aria-controls="links-accordion-header"
                            aria-expanded="false"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                    >
                        Links to NCBI Databases
                    </button>
                    <div data-source-url="/pmc/resources/db-links/9857861/" class="usa-accordion-content" id="links-accordion-header" aria-hidden="true"></div>
                </div>
            </li>

            
        
        </ul>
    </section>

 </div>
            </div>
        

    </nav>

</header>

    
    

    <div class="usa-overlay"></div>
    
<main id="main-content" class="usa-grid usa-layout-docs pmc-main">
    <article class="usa-width-three-fourths usa-layout-docs-main_content pmc-article">
        <section class="usa-breadcrumb usa-breadcrumb--wrap">
         
    <ul class="usa-breadcrumb__list">
            
                <li class="usa-breadcrumb__list-item"
                    >
                    <a href="/pmc/journals/" class="navlink">Journal List</a>
                </li>
            
                <li class="usa-breadcrumb__list-item"
                    >
                    <a href="/pmc/?term=%22Entropy%20(Basel)%22[journal]" class="navlink">Entropy (Basel)</a>
                </li>
            
                <li class="usa-breadcrumb__list-item"
                     aria-current="page" >
                    PMC9857861
                </li>
            
    </ul>
 

        </section>
        
  <div class="pmc-article__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/pmc/about/disclaimer/">PMC Disclaimer</a>
    |
    <a data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/pmc/about/copyright/">
        PMC Copyright Notice
    </a>
</div>

        <section class="pmc-page-banner" role="banner">
            
                
                    <div><img src="/corehtml/pmc/pmcgifs/logo-entropy.jpg" alt="Logo of entropy" usemap="#logo-imagemap" /><map id="logo-imagemap" name="logo-imagemap"><area alt="Link to Publisher's site" title="Link to Publisher's site" shape="default" coords="0,0,499,74" href="https://www.mdpi.com/journal/entropy" target="_blank" rel="noopener noreferrer" ref="reftype=publisher&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CBanner&amp;TO=Publisher%7COther%7CN/A" /></map></div> 
                
            
        </section>
        <section  role="document">
            
                <div id="mc" class=" article lit-style content pmc-wm slang-all page-box"><!--main-content--><div class="jig-ncbiinpagenav" data-jigconfig="smoothScroll: false, allHeadingLevels: ['h2'], headingExclude: ':hidden,.nomenu'"><div class="fm-sec half_rhythm no_top_margin"><div class="fm-flexbox"><div class="fm-citation"><div class="citation-default"><div class="part1"><span id="pmcmata">Entropy (Basel).</span> 2023 Jan; 25(1): 174. </div><div class="part2"><span class="fm-vol-iss-date">Published online 2023 Jan 15. </span>  <span class="doi"><span>doi: </span><a href="//doi.org/10.3390%2Fe25010174" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CFront%20Matter&amp;TO=Content%20Provider%7CCrosslink%7CDOI">10.3390/e25010174</a></span></div></div></div><div class="fm-ids"><div class="fm-citation-pmcid"><span class="fm-citation-ids-label">PMCID: </span><span>PMC9857861</span></div><div class="fm-citation-pmid">PMID: <a href="https://pubmed.ncbi.nlm.nih.gov/36673315">36673315</a></div></div></div><h1 class="content-title">Long-Range Dependence Involutional Network for Logo Detection</h1><div class="half_rhythm"><div class="contrib-group fm-author"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=Li%20X%5BAuthor%5D">Xingzhuo Li</a>, <span class="fm-role">Conceptualization</span>, <span class="fm-role">Methodology</span>, <span class="fm-role">Software</span>, <span class="fm-role">Validation</span>, <span class="fm-role">Formal analysis</span>, <span class="fm-role">Investigation</span>, <span class="fm-role">Data curation</span>, <span class="fm-role">Writing &#x02013; original draft</span>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=Hou%20S%5BAuthor%5D">Sujuan Hou</a>, <span class="fm-role">Conceptualization</span>, <span class="fm-role">Methodology</span>, <span class="fm-role">Resources</span>, <span class="fm-role">Writing &#x02013; review &#x00026; editing</span>, <span class="fm-role">Project administration</span>, <span class="fm-role">Funding acquisition</span>,<sup>*</sup> <a href="https://pubmed.ncbi.nlm.nih.gov/?term=Zhang%20B%5BAuthor%5D">Baisong Zhang</a>, <span class="fm-role">Validation</span>, <span class="fm-role">Formal analysis</span>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=Wang%20J%5BAuthor%5D">Jing Wang</a>, <span class="fm-role">Writing &#x02013; review &#x00026; editing</span>, <span class="fm-role">Visualization</span>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=Jia%20W%5BAuthor%5D">Weikuan Jia</a>, <span class="fm-role">Writing &#x02013; review &#x00026; editing</span>, and  <a href="https://pubmed.ncbi.nlm.nih.gov/?term=Zheng%20Y%5BAuthor%5D">Yuanjie Zheng</a>, <span class="fm-role">Resources</span>, <span class="fm-role">Supervision</span>, <span class="fm-role">Funding acquisition</span></div><div style="display:none" class="contrib-group aff-tip"></div></div><div class="contrib-group half_rhythm fm-editor">Andrea Prati, <span class="fm-role">Academic Editor</span>, Luis Javier Garc&#x000ed;a Villalba, <span class="fm-role">Academic Editor</span>, and  Vincent A. Cicirello, <span class="fm-role">Academic Editor</span></div><div class="half_rhythm"><div class="togglers fm-copyright-license"><a href="#" class="pmctoggle" rid="idm139848927060448_ai idm139848935906608_ai idm139848925456432_ai">Author information</a> <a href="#" class="pmctoggle" rid="idm139848927060448_an">Article notes</a> <a href="#" class="pmctoggle" rid="idm139848927060448_cpl">Copyright and License information</a> <a href="/pmc/about/disclaimer/" style="margin-left: 1em">PMC Disclaimer</a></div><div class="fm-authors-info hide half_rhythm" id="idm139848927060448_ai" style="display:none"><div class="fm-affl" id="af1-entropy-25-00174">School of Information Science and Engineering, Shandong Normal University, Jinan 250358, China</div><div id="c1-entropy-25-00174"><sup>*</sup>Correspondence: <a href="mailto:dev@null" data-email="nc.ude.unds@uohnaujus" class="oemail">nc.ude.unds@uohnaujus</a></div></div><div class="fm-article-notes hide half_rhythm" id="idm139848927060448_an" style="display:none"><div class="fm-pubdate half_rhythm">Received 2022 Nov 22; Revised 2023 Jan 11; Accepted 2023 Jan 13.</div></div><div class="permissions half_rhythm hide" id="idm139848927060448_cpl" style="display:none"><div class="fm-copyright half_rhythm"><a href="/pmc/about/copyright/">Copyright</a> &#x000a9; 2023 by the authors.</div><div class="license half_rhythm">Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<a href="https://creativecommons.org/licenses/by/4.0/" data-ga-action="click_feat_suppl" ref="reftype=extlink&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CFront%20Matter&amp;TO=External%7CLink%7CURI" target="_blank">https://creativecommons.org/licenses/by/4.0/</a>).</div></div></div><div id="pmclinksbox" class="links-box whole_rhythm hidden" role="complementary" aria-label="Related or updated information about this article."></div></div><div class="sec"></div><div id="ass-data" class="tsec fm-sec whole_rhythm" data-section="Featured_PMC_Datacitation"><h2 class="nomenu">Associated Data</h2><dl data-length="42" class="box-data-avail whole_rhythm no_bottom_margin"><dt><a href="#" rid="data-avl-stmnt" data-ga-action="click_feat_toggler" data-ga-label="Data Availability Statement" class="pmctoggle">Data Availability Statement</a></dt><dd id="data-avl-stmnt" style="display: none;"><p>The data are contained within the article.</p></dd></dl></div><div id="abstract-a.ad.b.p" lang="en" class="tsec sec"><h2 class="head no_bottom_margin" id="abstract-a.ad.b.ptitle">Abstract</h2><!--article-meta--><div><p class="p p-first-last">Logo detection is one of the crucial branches in computer vision due to various real-world applications, such as automatic logo detection and recognition, intelligent transportation, and trademark infringement detection. Compared with traditional handcrafted-feature-based methods, deep learning-based convolutional neural networks (CNNs) can learn both low-level and high-level image features. Recent decades have witnessed the great feature representation capabilities of deep CNNs and their variants, which have been very good at discovering intricate structures in high-dimensional data and are thereby applicable to many domains including logo detection. However, logo detection remains challenging, as existing detection methods cannot solve well the problems of a multiscale and large aspect ratios. In this paper, we tackle these challenges by developing a novel long-range dependence involutional network (LDI-Net). Specifically, we designed a strategy that combines a new operator and a self-attention mechanism via rethinking the intrinsic principle of convolution called long-range dependence involution (LD involution) to alleviate the detection difficulties caused by large aspect ratios. We also introduce a multilevel representation neural architecture search (MRNAS) to detect multiscale logo objects by constructing a novel multipath topology. In addition, we implemented an adaptive RoI pooling module (ARM) to improve detection efficiency by addressing the problem of logo deformation. Comprehensive experiments on four benchmark logo datasets demonstrate the effectiveness and efficiency of the proposed approach.</p></div><div class="sec"><strong class="kwd-title">Keywords: </strong><span class="kwd-text">object detection, logo detection, feature fusion, attention mechanism</span></div></div><div id="sec1-entropy-25-00174" class="tsec sec"><h2 class="head no_bottom_margin" id="sec1-entropy-25-00174title">1. Introduction</h2><p class="p p-first">Feature extraction is the most fundamental problem in various image-related tasks. Recent years have witnessed the powerful feature representation capability of convolutional neural networks (CNNs) that makes them very good at extracting rich image features. This is because they have two inherent advantages, namely, sparse connectivity and weight sharing. The former changes the situation of full connectivity in traditional neural networks and enables local perception, while the latter allows for the number of parameters to be significantly reduced, thus allowing for CNNs to train light models with fewer parameters. These advantages enable CNNs to outperform the traditional manual feature method. Therefore, deep-learning-based CNNs are increasingly dominant in object detection tasks. As a special form of object detection, logo detection aims at finding all the logos in an image or a video and return their locations. Extracting effective features is a crucial step in logo detection, in which deep CNNs can help in discovering intricate structures in logo datasets.</p><p>Logo detection plays an important role in various real-world applications, such as intelligent transportation [<a href="#B1-entropy-25-00174" rid="B1-entropy-25-00174" class=" bibr popnode">1</a>,<a href="#B2-entropy-25-00174" rid="B2-entropy-25-00174" class=" bibr popnode">2</a>], trademark infringement detection [<a href="#B3-entropy-25-00174" rid="B3-entropy-25-00174" class=" bibr popnode">3</a>], and automatic logo detection and recognition [<a href="#B4-entropy-25-00174" rid="B4-entropy-25-00174" class=" bibr popnode">4</a>]. However, logo detection is a complex task compared to general object detection because logo images usually have two distinctive characteristics: a large aspect ratio and multiple scales. On the one hand, there are many long words, artistic words, and feature images in a logo image resulting in a relatively large aspect ratio. On the other hand, the exquisite design of logo images causes a variety of multiscale logo objects in an image.</p><p>Compared with general object detection, the challenges of logo detection mainly come from two aspects:</p><ul class="unordered" style="list-style-type:disc"><li><div>The large aspect ratio of logos usually spans a large area in an image, as shown in <a href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f001/" target="figure" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-f001" rid-ob="ob-entropy-25-00174-f001" co-legend-rid="lgnd_entropy-25-00174-f001"><span>Figure 1</span></a>a. As far as we know, there has been little research on the problem of large aspect ratios in logo detection. Existing two-stage approaches based on fixed-size anchors fail to complete the detection of flexible aspect ratio logos [<a href="#B5-entropy-25-00174" rid="B5-entropy-25-00174" class=" bibr popnode">5</a>,<a href="#B6-entropy-25-00174" rid="B6-entropy-25-00174" class=" bibr popnode">6</a>,<a href="#B7-entropy-25-00174" rid="B7-entropy-25-00174" class=" bibr popnode">7</a>,<a href="#B8-entropy-25-00174" rid="B8-entropy-25-00174" class=" bibr popnode">8</a>]. Optimized strategies [<a href="#B9-entropy-25-00174" rid="B9-entropy-25-00174" class=" bibr popnode">9</a>,<a href="#B10-entropy-25-00174" rid="B10-entropy-25-00174" class=" bibr popnode">10</a>,<a href="#B11-entropy-25-00174" rid="B11-entropy-25-00174" class=" bibr popnode">11</a>] also have a limited effect on the detection of logos with a large aspect ratio. The method in [<a href="#B12-entropy-25-00174" rid="B12-entropy-25-00174" class=" bibr popnode">12</a>] could generate anchors of any shape, but it was unable to extract long-range dependence. Similarly, the traditional convolutional approach cannot fully utilize long-range interaction and the locations of various spatial features, which severely restricts its ability to address the large aspect ratio of logos.</div><!--fig ft0--><!--fig mode=article f1--><div class="fig iconblock whole_rhythm" id="entropy-25-00174-f001" co-legend-rid="lgnd_entropy-25-00174-f001"><a href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f001/" target="figure" rid-figpopup="entropy-25-00174-f001" rid-ob="ob-entropy-25-00174-f001"><!--fig/graphic|fig/alternatives/graphic mode="anchored" m1--><div class="figure" data-largeobj="" data-largeobj-link-rid="largeobj_idm139848925383872"><a class="inline_block ts_canvas" href="/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=9857861_entropy-25-00174-g001.jpg" target="tileshopwindow" rel="noopener"><div class="ts_bar small" title="Click on image to zoom"></div><img loading="lazy" alt="An external file that holds a picture, illustration, etc.&#10;Object name is entropy-25-00174-g001.jpg" title="Click on image to zoom" class="tileshop" src="/pmc/articles/PMC9857861/bin/entropy-25-00174-g001.jpg" /></a></div><div class="largeobj-link align_right" id="largeobj_idm139848925383872"><a target="object" rel="noopener" href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f001/?report=objectonly">Open in a separate window</a></div></a><div class="icnblk_cntnt" id="lgnd_entropy-25-00174-f001"><div><a class="figpopup" href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f001/" target="figure" rid-figpopup="entropy-25-00174-f001" rid-ob="ob-entropy-25-00174-f001">Figure 1</a></div><!--caption a7--><div class="caption"><p>Three logo challenges. (<strong>a</strong>) Logo with a large aspect ratio; (<strong>b</strong>) logos with multiple scales in an image; (<strong>c</strong>) logo deformation caused by angle change, reflection, and other reasons.</p></div></div></div></li><li><div>Multiscale logo objects in an image. As seen in <a href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f001/" target="figure" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-f001" rid-ob="ob-entropy-25-00174-f001" co-legend-rid="lgnd_entropy-25-00174-f001"><span>Figure 1</span></a>b, &#x02019;adidas&#x02019; appears both in the foreground and background, but the scale varies greatly. Scale diversity can be resolved utilizing feature pyramid networks (FPNs) [<a href="#B13-entropy-25-00174" rid="B13-entropy-25-00174" class=" bibr popnode">13</a>], but the semantic information of small objects may be lost after multiple instances of downsampling. The bottom&#x02013;up information channel is increased by PANet [<a href="#B14-entropy-25-00174" rid="B14-entropy-25-00174" class=" bibr popnode">14</a>], but the information is concentrated more in the adjacent layers. Although SEPC [<a href="#B15-entropy-25-00174" rid="B15-entropy-25-00174" class=" bibr popnode">15</a>] can extract multilevel features, it has the disadvantage of the topology being too simple to extract more information.</div></li></ul><p></p><p>In this paper, we present a novel logo detection method called long-range dependence involutional Network (LDI-Net). We rethink the intrinsic principle of convolution, and propose long-range dependence involution (LD involution) and apply it to a region proposal network (RPN). Two major convolutional flaws are remedied by LD involution, since it has significant advantages in acquiring long-range interactions in spatial and channel dimensions. Meanwhile, it can preferentially extract significant visual information in space by creating particular involutional kernels for certain spatial locations. The construction of LD involution enables visual information and elements in the spatial domain to be reasonably allocated and sorted on the logo image to the greatest extent. The channel-sharing involutional kernel allows for us to use a larger K to satisfy the establishment and correlation of long-range information, and significantly reduces the redundancy of the model. For logo detection, a logo image with a very large aspect ratio is characterized by high requirements for long-distance information contact. LDI-Net improves the detection performance of logos with a large aspect ratio by employing a new operator and a self-attention mechanism. For the second issue, we suggest a multilevel representation neural architecture search (MRNAS) to detect multiscale logo objects. MRNAS introduces six heterogeneous information paths to construct a diverse multipath topology that combines semantic information and location representation, optimizing cross-level interaction between features. Additionally, we implemented an adaptable RoI pooling module (ARM) to improve detection efficiency and achieve adaptive feature learning for differently shaped objects. By adding additional offset and a modulation mechanism, the logo deformation problem caused by angles, occlusion, rotation, distortion, reflection, etc. (as shown in <a href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f001/" target="figure" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-f001" rid-ob="ob-entropy-25-00174-f001" co-legend-rid="lgnd_entropy-25-00174-f001"><span>Figure 1</span></a>c) is solved.</p><p class="p p-last">The main contributions of this paper can be summarized as follows:</p><ul class="unordered" style="list-style-type:disc"><li><div>We developed a network with LD Involution for logo detection by establishing long-range information dependence, and ranking the significance of visual information via a new operator and a self-attention mechanism to solve the problem of a large aspect ratio.</div></li><li><div>We constructed a diverse multipath topology on the basis of neural architecture search theory in which each path utilizes a specific feature fusion.</div></li><li><div>We conducted extensive experiments and evaluated our approach on four benchmark logo datasets: FlickrLogos-32, QMUL-OpenLogo, LogoDet-3K-1000 and LogoDet-3K. The experimental results demonstrate the effectiveness of the proposed model.</div></li></ul><p></p></div><div id="sec2-entropy-25-00174" class="tsec sec"><h2 class="head no_bottom_margin" id="sec2-entropy-25-00174title">2. Related Work</h2><div id="sec2dot1-entropy-25-00174" class="sec sec-first"><h3 id="sec2dot1-entropy-25-00174title">2.1. Object Detection</h3><p class="p p-first">In recent years, CNNs have been widely used in deep learning and have achieved many good research results [<a href="#B16-entropy-25-00174" rid="B16-entropy-25-00174" class=" bibr popnode">16</a>,<a href="#B17-entropy-25-00174" rid="B17-entropy-25-00174" class=" bibr popnode">17</a>,<a href="#B18-entropy-25-00174" rid="B18-entropy-25-00174" class=" bibr popnode">18</a>]. Object detection is one of the most fundamental and challenging problems in computer vision, and has received much attention in recent years. In the era of deep learning, object detection methods are divided into two genres: two-stage and one-stage. The two-stage system first creates regional proposals on the basis of image content, followed by categorization and localization. Classical two-stage algorithms include fast R-CNN [<a href="#B19-entropy-25-00174" rid="B19-entropy-25-00174" class=" bibr popnode">19</a>], faster R-CNN [<a href="#B5-entropy-25-00174" rid="B5-entropy-25-00174" class=" bibr popnode">5</a>], and cascade R-CNN [<a href="#B20-entropy-25-00174" rid="B20-entropy-25-00174" class=" bibr popnode">20</a>]. One-stage algorithms are characterized by one-step completion without regional proposals, directly generating the category and location coordinates, such as the YOLO series [<a href="#B8-entropy-25-00174" rid="B8-entropy-25-00174" class=" bibr popnode">8</a>,<a href="#B21-entropy-25-00174" rid="B21-entropy-25-00174" class=" bibr popnode">21</a>,<a href="#B22-entropy-25-00174" rid="B22-entropy-25-00174" class=" bibr popnode">22</a>]. Among them, faster R-CNN is a milestone work based on RPN.</p><p class="p p-last">It is an important issue for object detection to recognize multiscale objects. Many works were improved on the basis of FPN [<a href="#B13-entropy-25-00174" rid="B13-entropy-25-00174" class=" bibr popnode">13</a>], including PANet [<a href="#B14-entropy-25-00174" rid="B14-entropy-25-00174" class=" bibr popnode">14</a>], BiFPN [<a href="#B23-entropy-25-00174" rid="B23-entropy-25-00174" class=" bibr popnode">23</a>], and SEPC [<a href="#B15-entropy-25-00174" rid="B15-entropy-25-00174" class=" bibr popnode">15</a>], because of its strong performance in multilevel feature extraction. PANet enhanced the representation ability by integrating bottom&#x02013;up and top&#x02013;down paths. BiFPN introduced learnable weights to determine the importance of different input features, and repeatedly employed multiscale feature fusion. SEPC performed deformable convolution on the high-level features of a feature pyramid, which adapted to the actual scale change and maintained scale balance between layers. Although these methods implemented the information interaction between multiple layers, the relatively simple topology of the search structure lacked the feature information of small objects.</p></div><div id="sec2dot2-entropy-25-00174" class="sec sec-last"><h3 id="sec2dot2-entropy-25-00174title">2.2. Logo Detection</h3><p class="p p-first">Logo detection has been extensively studied in e-commerce and multimedia fields [<a href="#B24-entropy-25-00174" rid="B24-entropy-25-00174" class=" bibr popnode">24</a>,<a href="#B25-entropy-25-00174" rid="B25-entropy-25-00174" class=" bibr popnode">25</a>,<a href="#B26-entropy-25-00174" rid="B26-entropy-25-00174" class=" bibr popnode">26</a>,<a href="#B27-entropy-25-00174" rid="B27-entropy-25-00174" class=" bibr popnode">27</a>]. Early logo detection was generally completed on the basis of manual features and traditional classification models, such as Viola&#x02013;Jones (VJ) [<a href="#B28-entropy-25-00174" rid="B28-entropy-25-00174" class=" bibr popnode">28</a>], the histogram of oriented gradients (HOG) [<a href="#B29-entropy-25-00174" rid="B29-entropy-25-00174" class=" bibr popnode">29</a>], and the deformable parts model (DPM) [<a href="#B30-entropy-25-00174" rid="B30-entropy-25-00174" class=" bibr popnode">30</a>]. Yan et al. [<a href="#B31-entropy-25-00174" rid="B31-entropy-25-00174" class=" bibr popnode">31</a>] used the Bayesian classifier framework to detect and remove video logos. Wang et al. [<a href="#B32-entropy-25-00174" rid="B32-entropy-25-00174" class=" bibr popnode">32</a>] implemented a simple automotive logo recognition method using template matching and edge orientation histograms.</p><p>In the last few years, deep-learning-based logo detection algorithms have become mainstream. Bao et al. [<a href="#B33-entropy-25-00174" rid="B33-entropy-25-00174" class=" bibr popnode">33</a>] directly applied faster R-CNN to logo detection and achieved good performance. Xu et al. [<a href="#B27-entropy-25-00174" rid="B27-entropy-25-00174" class=" bibr popnode">27</a>] proposed a solution to robust defence competition in e-commerce logo detection. Velazquez et al. [<a href="#B34-entropy-25-00174" rid="B34-entropy-25-00174" class=" bibr popnode">34</a>] improved the detection performance of small objects by incorporating FPN into the DETR structure. Wang et al. [<a href="#B25-entropy-25-00174" rid="B25-entropy-25-00174" class=" bibr popnode">25</a>] built the largest fully annotated logo detection dataset, i.e., LogoDet-3K, and proposed Logo-Yolo to resolve the imbalanced samples of logo objects. A cross-view learning method [<a href="#B35-entropy-25-00174" rid="B35-entropy-25-00174" class=" bibr popnode">35</a>] provided ideas for logo detection. Hou et al. [<a href="#B26-entropy-25-00174" rid="B26-entropy-25-00174" class=" bibr popnode">26</a>] constructed a large dataset, FoodLogoDet-1500, to address data limitations in food logo detection, and proposed MFDNet to address multiscale and similar logo problems.</p><p class="p p-last">Different from previous work, we rethought the intrinsic principle of convolution and applied the proposed LD involution to RPN. Meanwhile, we constructed a diverse multipath topology on the basis of neural architecture search theory in which each path utilized a specific feature fusion. In addition, we introduced ARM to achieve adaptive feature learning for different objects.</p></div></div><div id="sec3-entropy-25-00174" class="tsec sec"><h2 class="head no_bottom_margin" id="sec3-entropy-25-00174title">3. Our Approach</h2><p class="p p-first">In this section, we present logo detection method LDI-Net, and the overall framework is shown in <a href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f002/" target="figure" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-f002" rid-ob="ob-entropy-25-00174-f002" co-legend-rid="lgnd_entropy-25-00174-f002"><span>Figure 2</span></a>. Specifically, the model first feeds the feature map into MRNAS to learn multilevel features after extracting essential features from the input image. Next, it feeds the feature map into RPN, established by LD involution to obtain higher-quality regional proposals. Then, it feeds the feature map into ARM to enhance the modeling capability. Lastly, the model performs classification and localization. All components are described in detail in the following sections.</p><!--fig ft0--><!--fig mode=article f1--><div class="fig iconblock whole_rhythm" id="entropy-25-00174-f002" co-legend-rid="lgnd_entropy-25-00174-f002"><a href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f002/" target="figure" rid-figpopup="entropy-25-00174-f002" rid-ob="ob-entropy-25-00174-f002"><!--fig/graphic|fig/alternatives/graphic mode="anchored" m1--><div class="figure" data-largeobj="" data-largeobj-link-rid="largeobj_idm139848934404400"><a class="inline_block ts_canvas" href="/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=9857861_entropy-25-00174-g002.jpg" target="tileshopwindow" rel="noopener"><div class="ts_bar small" title="Click on image to zoom"></div><img loading="lazy" alt="An external file that holds a picture, illustration, etc.&#10;Object name is entropy-25-00174-g002.jpg" title="Click on image to zoom" class="tileshop" src="/pmc/articles/PMC9857861/bin/entropy-25-00174-g002.jpg" /></a></div><div class="largeobj-link align_right" id="largeobj_idm139848934404400"><a target="object" rel="noopener" href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f002/?report=objectonly">Open in a separate window</a></div></a><div class="icnblk_cntnt" id="lgnd_entropy-25-00174-f002"><div><a class="figpopup" href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f002/" target="figure" rid-figpopup="entropy-25-00174-f002" rid-ob="ob-entropy-25-00174-f002">Figure 2</a></div><!--caption a7--><div class="caption"><p>Overview of proposed LDI-Net for logo detection. MRNAS: multilevel representation neural architecture search. LD involution: long-range dependence involution. ARM: adaptive RoI pooling module.</p></div></div></div><div id="sec3dot1-entropy-25-00174" class="sec"><h3 id="sec3dot1-entropy-25-00174title">3.1. Multilevel Representation Neural Architecture Search</h3><p class="p p-first-last">As shown in <a href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f002/" target="figure" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-f002" rid-ob="ob-entropy-25-00174-f002" co-legend-rid="lgnd_entropy-25-00174-f002"><span>Figure 2</span></a>, the main body of MRNAS is a fully connected directed acyclic graph composed of <em>N</em> + 2 nodes, while <em>N</em> is a predefined constant value. In LDI-Net, to balance efficiency and accuracy, we set <em>N</em> to 5. The nodes of the directed acyclic graph represent the feature map driven by the feature pyramid, <em>P</em> is the input node, <em>O</em> is the output node, and <math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm1" overflow="scroll"><mrow><mrow><msub><mi>t</mi><mi>i</mi></msub><mrow><mo>(</mo><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>&#x02026;</mo><mo>,</mo><mi>N</mi><mo>)</mo></mrow></mrow></mrow></math> is the intermediate node. Different information paths are used as connections between the two nodes. We introduced six kinds of heterogeneous information paths: top&#x02013;down, bottom&#x02013;up, fusing&#x02013;splitting, scale&#x02013;equalizing, skip&#x02013;connect, and none [<a href="#B36-entropy-25-00174" rid="B36-entropy-25-00174" class=" bibr popnode">36</a>]. They could realize the aggregated combination of multilevel information on different paths. These information paths <math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm2" overflow="scroll"><mrow><mrow><mrow><mi>P</mi><mi>A</mi></mrow><mo>(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo>)</mo></mrow></mrow></math> transform <math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm3" overflow="scroll"><mrow><msub><mi>t</mi><mi>i</mi></msub></mrow></math> into <math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm4" overflow="scroll"><mrow><msub><mi>t</mi><mi>j</mi></msub></mrow></math>, and each node <math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm5" overflow="scroll"><mrow><mrow><mi>i</mi><mo>&#x02208;</mo></mrow></mrow></math><math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm6" overflow="scroll"><mrow><mfenced separators="" open="{" close="}"><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>&#x02026;</mo><mo>,</mo><mi>N</mi></mfenced></mrow></math> aggregates the input of the previous node:</p><div class="disp-formula" id="FD1-entropy-25-00174"><div class="f"><math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm7" display="block" overflow="scroll"><mrow><mrow><msub><mi>t</mi><mi>j</mi></msub><mo>=</mo><munder><mo>&#x02211;</mo><mrow><mi>i</mi><mo>&#x0003c;</mo><mi>j</mi></mrow></munder><mrow><mi>P</mi><mi>A</mi></mrow><mrow><mo>(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo>)</mo></mrow><mfenced separators="" open="(" close=")"><msub><mi>t</mi><mi>i</mi></msub></mfenced></mrow></mrow></math></div><div class="l">(1)</div></div><p></p></div><div id="sec3dot2-entropy-25-00174" class="sec"><h3 id="sec3dot2-entropy-25-00174title">3.2. Long-Range Dependence Involution</h3><p class="p p-first">The purpose of RPN is to generate regional proposals when detecting objects. LD involution is a more efficient way to correlate information compared with convolution, which improves the quality of generated candidate regions better than RPN.</p><p>Similar to involution [<a href="#B37-entropy-25-00174" rid="B37-entropy-25-00174" class=" bibr popnode">37</a>], the feature transformation process of LD involution is shown in <a href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f003/" target="figure" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-f003" rid-ob="ob-entropy-25-00174-f003" co-legend-rid="lgnd_entropy-25-00174-f003"><span>Figure 3</span></a>. For a coordinate point in the input feature map, its feature vector is first transformed by two steps of generation (as given in <a href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f004/" target="figure" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-f004" rid-ob="ob-entropy-25-00174-f004" co-legend-rid="lgnd_entropy-25-00174-f004"><span>Figure 4</span></a>) and each reshaped to expand into the involutional kernel corresponding to the coordinate point. Then, it multiadds with the <math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm8" overflow="scroll"><mrow><mrow><mi>K</mi><mo>&#x000d7;</mo><mi>K</mi></mrow></mrow></math> neighborhood near the coordinate point to obtain the final output feature map.</p><!--fig ft0--><!--fig mode=article f1--><div class="fig iconblock whole_rhythm" id="entropy-25-00174-f003" co-legend-rid="lgnd_entropy-25-00174-f003"><a href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f003/" target="figure" rid-figpopup="entropy-25-00174-f003" rid-ob="ob-entropy-25-00174-f003"><!--fig/graphic|fig/alternatives/graphic mode="anchored" m1--><div class="figure" data-largeobj="" data-largeobj-link-rid="largeobj_idm139848934401808"><a class="inline_block ts_canvas" href="/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=9857861_entropy-25-00174-g003.jpg" target="tileshopwindow" rel="noopener"><div class="ts_bar small" title="Click on image to zoom"></div><img loading="lazy" alt="An external file that holds a picture, illustration, etc.&#10;Object name is entropy-25-00174-g003.jpg" title="Click on image to zoom" class="tileshop" src="/pmc/articles/PMC9857861/bin/entropy-25-00174-g003.jpg" /></a></div><div class="largeobj-link align_right" id="largeobj_idm139848934401808"><a target="object" rel="noopener" href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f003/?report=objectonly">Open in a separate window</a></div></a><div class="icnblk_cntnt" id="lgnd_entropy-25-00174-f003"><div><a class="figpopup" href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f003/" target="figure" rid-figpopup="entropy-25-00174-f003" rid-ob="ob-entropy-25-00174-f003">Figure 3</a></div><!--caption a7--><div class="caption"><p>Feature map transformation process based on involution.</p></div></div></div><!--fig ft0--><!--fig mode=article f1--><div class="fig iconblock whole_rhythm" id="entropy-25-00174-f004" co-legend-rid="lgnd_entropy-25-00174-f004"><a href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f004/" target="figure" rid-figpopup="entropy-25-00174-f004" rid-ob="ob-entropy-25-00174-f004"><!--fig/graphic|fig/alternatives/graphic mode="anchored" m1--><div class="figure" data-largeobj="" data-largeobj-link-rid="largeobj_idm139848934399216"><a class="inline_block ts_canvas" href="/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=9857861_entropy-25-00174-g004.jpg" target="tileshopwindow" rel="noopener"><div class="ts_bar small" title="Click on image to zoom"></div><img loading="lazy" alt="An external file that holds a picture, illustration, etc.&#10;Object name is entropy-25-00174-g004.jpg" title="Click on image to zoom" class="tileshop" src="/pmc/articles/PMC9857861/bin/entropy-25-00174-g004.jpg" /></a></div><div class="largeobj-link align_right" id="largeobj_idm139848934399216"><a target="object" rel="noopener" href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f004/?report=objectonly">Open in a separate window</a></div></a><div class="icnblk_cntnt" id="lgnd_entropy-25-00174-f004"><div><a class="figpopup" href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f004/" target="figure" rid-figpopup="entropy-25-00174-f004" rid-ob="ob-entropy-25-00174-f004">Figure 4</a></div><!--caption a7--><div class="caption"><p>Construction of the involutional kernel.</p></div></div></div><p class="p p-last">We focus on long-range dependence, which is crucial for the optimization of large aspect ratios. Inspired by [<a href="#B38-entropy-25-00174" rid="B38-entropy-25-00174" class=" bibr popnode">38</a>], we adopted a flexible generation method to generate the involutional kernel instead of the convolutional kernel. As shown in <a href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f004/" target="figure" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-f004" rid-ob="ob-entropy-25-00174-f004" co-legend-rid="lgnd_entropy-25-00174-f004"><span>Figure 4</span></a>, the involutional kernel was constructed in two parts. In the first part, we used global self-attention to extract distant information. In the content-position section, we utilized relative position encodings <math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm9" overflow="scroll"><mrow><msub><mi>R</mi><mi>h</mi></msub></mrow></math> and <math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm10" overflow="scroll"><mrow><msub><mi>R</mi><mi>w</mi></msub></mrow></math> to represent height and width, respectively. We used <em>q</em>, <em>k</em>, and <em>r</em> to represent query, key, and position encoding, respectively. Attention logits are denoted as <math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm11" overflow="scroll"><mrow><mrow><mi>q</mi><msup><mi>k</mi><mi>T</mi></msup><mo>+</mo><mi>q</mi><msup><mi>r</mi><mi>T</mi></msup></mrow></mrow></math>. &#x02a01; and &#x02a02; represent element-wise and matrix multiplication, respectively. After self-attention, global average pooling was employed to refine the context modeling and enrich the extraction of long-range information. The second part is to capture channel dependence by learning the correlation between channels and filtering attention to the channel. After the feature extraction of different positions in the first part, the channel feature dependence was successively obtained with <math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm12" overflow="scroll"><mrow><mrow><mn>1</mn><mo>&#x000d7;</mo><mn>1</mn></mrow></mrow></math> convolution, BN, ReLU, and <math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm13" overflow="scroll"><mrow><mrow><mn>1</mn><mo>&#x000d7;</mo><mn>1</mn></mrow></mrow></math> convolution. Combined with the general form described above, the module is defined as follows:</p><div class="disp-formula" id="FD2-entropy-25-00174"><div class="f"><math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm14" display="block" overflow="scroll"><mrow><mrow><msub><mi>M</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><msub><mi>W</mi><mi>s</mi></msub><mi>&#x003b4;</mi><msub><mi>W</mi><mi>f</mi></msub><mrow><mo>(</mo><mi>G</mi><mrow><mo>(</mo><mi>S</mi><mrow><mo>(</mo><msub><mi>X</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>)</mo></mrow><mo>)</mo></mrow><mo>)</mo></mrow></mrow></mrow></math></div><div class="l">(2)</div></div><p>
where <math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm15" overflow="scroll"><mrow><msub><mi>X</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow></math> and <math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm16" overflow="scroll"><mrow><msub><mi>M</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow></math> represent input and output, respectively. <em>S</em> represents the global self-attention, while <em>G</em> represents global average pooling. <math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm17" overflow="scroll"><mrow><msub><mi>W</mi><mi>s</mi></msub></mrow></math> and <math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm18" overflow="scroll"><mrow><msub><mi>W</mi><mi>f</mi></msub></mrow></math> represent the linear transformation matrix (<math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm19" overflow="scroll"><mrow><mrow><mn>1</mn><mo>&#x000d7;</mo><mn>1</mn></mrow></mrow></math> convolution was adopted here), while <math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm20" overflow="scroll"><mrow><mi>&#x003b4;</mi></mrow></math> represents BN and ReLU.</p></div><div id="sec3dot3-entropy-25-00174" class="sec"><h3 id="sec3dot3-entropy-25-00174title">3.3. Adaptive RoI Pooling Module</h3><p class="p p-first">RoI pooling is used to pool arbitrary-size input feature maps into the same size feature maps. RoI pooling divides RoI into <em>i</em> bins. Each bin can be formulated as follows:</p><div class="disp-formula" id="FD3-entropy-25-00174"><div class="f"><math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm21" display="block" overflow="scroll"><mrow><mrow><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow><mo>=</mo><munderover><mo>&#x02211;</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>m</mi><mi>i</mi></msub></munderover><mi>x</mi><mrow><mo>(</mo><msub><mi>R</mi><mrow><mi>i</mi><mi>t</mi></mrow></msub><mo>)</mo></mrow><mo>/</mo><msub><mi>m</mi><mi>i</mi></msub></mrow></mrow></math></div><div class="l">(3)</div></div><p>
where <em>x</em> is the input feature map, and <em>y</em> is the output feature map. <math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm22" overflow="scroll"><mrow><msub><mi>R</mi><mrow><mi>i</mi><mi>t</mi></mrow></msub></mrow></math> is the sampling position of the <math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm23" overflow="scroll"><mrow><mrow><mi>t</mi><mo>&#x02212;</mo><mi>t</mi><mi>h</mi></mrow></mrow></math> grid cell in the <math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm24" overflow="scroll"><mrow><mrow><mi>i</mi><mo>&#x02212;</mo><mi>t</mi><mi>h</mi></mrow></mrow></math> bin, and <math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm25" overflow="scroll"><mrow><msub><mi>m</mi><mi>i</mi></msub></mrow></math> is the number of grid cells in the bin. We summed the sampling values on the grid cell and took the average value to calculate the output of the bin.</p><p class="p p-last">As shown in <a href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f002/" target="figure" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-f002" rid-ob="ob-entropy-25-00174-f002" co-legend-rid="lgnd_entropy-25-00174-f002"><span>Figure 2</span></a>, in the adaptive RoI pooling, we added an additional offset and a modulation mechanism [<a href="#B39-entropy-25-00174" rid="B39-entropy-25-00174" class=" bibr popnode">39</a>]:</p><div class="disp-formula" id="FD4-entropy-25-00174"><div class="f"><math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm26" display="block" overflow="scroll"><mrow><mrow><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow><mo>=</mo><munderover><mo>&#x02211;</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>m</mi><mi>i</mi></msub></munderover><mi>x</mi><mrow><mo>(</mo><msub><mi>R</mi><mrow><mi>i</mi><mi>t</mi></mrow></msub><mo>+</mo><mo>&#x00394;</mo><msub><mi>R</mi><mi>i</mi></msub><mo>)</mo></mrow><mo>&#x000b7;</mo><mo>&#x00394;</mo><msub><mi>h</mi><mi>i</mi></msub><mo>/</mo><msub><mi>m</mi><mi>i</mi></msub></mrow></mrow></math></div><div class="l">(4)</div></div><p>
where <math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm27" overflow="scroll"><mrow><mrow><mo>&#x00394;</mo><msub><mi>R</mi><mi>i</mi></msub></mrow></mrow></math> is the offset that is used to increase the spatial sampling position and improve the feature extraction ability of the network. <math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm28" overflow="scroll"><mrow><mrow><mo>&#x00394;</mo><msub><mi>h</mi><mi>i</mi></msub></mrow></mrow></math> is the modulation scalar that is used to assign the weight to each offset corrected region.</p></div><div id="sec3dot4-entropy-25-00174" class="sec sec-last"><h3 id="sec3dot4-entropy-25-00174title">3.4. Loss Function</h3><p class="p p-first">In LDI-Net, the final loss function consists of <math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm29" overflow="scroll"><mrow><msub><mi>L</mi><mrow><mi>r</mi><mi>p</mi><mi>n</mi></mrow></msub></mrow></math>, <math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm30" overflow="scroll"><mrow><msub><mi>L</mi><mrow><mi>c</mi><mi>l</mi><mi>s</mi></mrow></msub></mrow></math> and <math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm31" overflow="scroll"><mrow><msub><mi>L</mi><mrow><mi>l</mi><mi>o</mi><mi>c</mi></mrow></msub></mrow></math>, as listed in Equation (<a href="#FD5-entropy-25-00174" rid="FD5-entropy-25-00174" class=" disp-formula">5</a>):</p><div class="disp-formula" id="FD5-entropy-25-00174"><div class="f"><math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm32" display="block" overflow="scroll"><mrow><mrow><mi>L</mi><mo>=</mo><msub><mi>L</mi><mrow><mi>r</mi><mi>p</mi><mi>n</mi></mrow></msub><mo>+</mo><msub><mi>L</mi><mrow><mi>c</mi><mi>l</mi><mi>s</mi></mrow></msub><mo>+</mo><msub><mi>L</mi><mrow><mi>l</mi><mi>o</mi><mi>c</mi></mrow></msub></mrow></mrow></math></div><div class="l">(5)</div></div><p>
where <math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm33" overflow="scroll"><mrow><msub><mi>L</mi><mrow><mi>r</mi><mi>p</mi><mi>n</mi></mrow></msub></mrow></math> is the RPN loss, <math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm34" overflow="scroll"><mrow><msub><mi>L</mi><mrow><mi>c</mi><mi>l</mi><mi>s</mi></mrow></msub></mrow></math> is the classification loss, and <math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm35" overflow="scroll"><mrow><msub><mi>L</mi><mrow><mi>l</mi><mi>o</mi><mi>c</mi></mrow></msub></mrow></math> is the boundary box regression loss.</p><p class="p p-last">We implemented <math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm36" overflow="scroll"><mrow><msub><mi>L</mi><mrow><mi>c</mi><mi>l</mi><mi>s</mi></mrow></msub></mrow></math> by the cross-entropy loss function. In order to better adapt the changes in distribution, we used Dynamic SmoothL1 Loss (DSL) in <math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm37" overflow="scroll"><mrow><msub><mi>L</mi><mrow><mi>l</mi><mi>o</mi><mi>c</mi></mrow></msub></mrow></math> to compensate for high-quality samples and pay more attention to high-quality samples:</p><div class="disp-formula" id="FD6-entropy-25-00174"><div class="f"><math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm38" display="block" overflow="scroll"><mrow><mrow><mi>D</mi><mi>S</mi><mi>L</mi><mrow><mo>(</mo><mi>a</mi><mo>,</mo><mi>&#x003c3;</mi><mo>)</mo></mrow><mo>=</mo><mfenced separators="" open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><msup><mrow><mn>0.5</mn><mo>|</mo><mi>a</mi><mo>|</mo></mrow><mn>2</mn></msup><mo>/</mo><mi>&#x003c3;</mi><mo>,</mo></mrow></mtd><mtd columnalign="right"><mrow><msup><mrow><mrow><mi>i</mi><mi>f</mi></mrow><mo>|</mo><mi>a</mi><mo>|</mo></mrow><mn>2</mn></msup><mo>&#x0003c;</mo><mi>&#x003c3;</mi><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign="left"><mrow><mo>|</mo><mi>a</mi><mo>|</mo><mo>&#x02212;</mo><mn>0.5</mn><mi>&#x003c3;</mi><mo>,</mo></mrow></mtd><mtd columnalign="right"><mrow><mi>o</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>r</mi><mi>w</mi><mi>i</mi><mi>s</mi><mi>e</mi><mo>.</mo></mrow></mtd></mtr></mtable></mfenced></mrow></mrow></math></div><div class="l">(6)</div></div><p></p></div></div><div id="sec4-entropy-25-00174" class="tsec sec"><h2 class="head no_bottom_margin" id="sec4-entropy-25-00174title">4. Experiments</h2><div id="sec4dot1-entropy-25-00174" class="sec sec-first"><h3 id="sec4dot1-entropy-25-00174title">4.1. Experimental Setting</h3><div id="sec4dot1dot1-entropy-25-00174" class="sec sec-first"><p></p><h4 id="sec4dot1dot1-entropy-25-00174title" class="inline">4.1.1. Datasets </h4><p class="p p-first-last">To evaluate the effectiveness of the proposed LDI-Net, we completed comprehensive experimental validation on four datasets: large-scale dataset LogoDet-3K [<a href="#B25-entropy-25-00174" rid="B25-entropy-25-00174" class=" bibr popnode">25</a>], medium-scale dataset LogoDet-3K-1000 [<a href="#B25-entropy-25-00174" rid="B25-entropy-25-00174" class=" bibr popnode">25</a>], and two small-scale datasets, QMUL-OpenLogo [<a href="#B40-entropy-25-00174" rid="B40-entropy-25-00174" class=" bibr popnode">40</a>] and FlickrLogos-32 [<a href="#B41-entropy-25-00174" rid="B41-entropy-25-00174" class=" bibr popnode">41</a>]. LogoDet-3K contains 158,652 pictures, including 142,142 for trainval and 16,510 for the test. LogoDet-3K-1000 is a subset of LogoDet-3K, sampled from LogoDet-3K. To further evaluate the generalization and robustness of the LDI-Net model, we also carried out extensive experiments on two widely used logo detection datasets, i.e., QMUL-OpenLogo and FlickrLogos-32. The detailed description of these datasets is shown in <a href="/pmc/articles/PMC9857861/table/entropy-25-00174-t001/" target="table" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-t001" rid-ob="ob-entropy-25-00174-t001" co-legend-rid=""><span>Table 1</span></a>. The classes, images and objects represent the number of categories, images and logos in the dataset, respectively. The trainval and test represent a division of the dataset whose sum is the number of images.</p><!--table ft1--><!--table-wrap mode="anchored" t5--><div class="table-wrap anchored whole_rhythm" id="entropy-25-00174-t001"><h3>Table 1</h3><!--caption a7--><div class="caption"><p>Statistics of four logo datasets.</p></div><div class="xtable"><table frame="hsides" rules="groups" class="rendered small default_table"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Datasets</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">#Classes</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">#Images</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">#Objects</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">#Trainval</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">#Test</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">FlickrLogos-32 [<a href="#B41-entropy-25-00174" rid="B41-entropy-25-00174" class=" bibr popnode">41</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">32</td><td align="center" valign="middle" rowspan="1" colspan="1">2240</td><td align="center" valign="middle" rowspan="1" colspan="1">3405</td><td align="center" valign="middle" rowspan="1" colspan="1">1478</td><td align="center" valign="middle" rowspan="1" colspan="1">762</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">QMUL-OpenLogo [<a href="#B40-entropy-25-00174" rid="B40-entropy-25-00174" class=" bibr popnode">40</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">352</td><td align="center" valign="middle" rowspan="1" colspan="1">27,083</td><td align="center" valign="middle" rowspan="1" colspan="1">51,207</td><td align="center" valign="middle" rowspan="1" colspan="1">18,752</td><td align="center" valign="middle" rowspan="1" colspan="1">8331</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LogoDet-3K-1000 [<a href="#B25-entropy-25-00174" rid="B25-entropy-25-00174" class=" bibr popnode">25</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">1000</td><td align="center" valign="middle" rowspan="1" colspan="1">85,344</td><td align="center" valign="middle" rowspan="1" colspan="1">101,345</td><td align="center" valign="middle" rowspan="1" colspan="1">75,785</td><td align="center" valign="middle" rowspan="1" colspan="1">9559</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LogoDet-3K [<a href="#B25-entropy-25-00174" rid="B25-entropy-25-00174" class=" bibr popnode">25</a>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">158,652</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">194,261</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">142,142</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16,510</td></tr></tbody></table></div><div class="largeobj-link align_right" id="largeobj_idm139848925228400"><a target="object" rel="noopener" href="/pmc/articles/PMC9857861/table/entropy-25-00174-t001/?report=objectonly">Open in a separate window</a></div></div></div><div id="sec4dot1dot2-entropy-25-00174" class="sec sec-last"><p></p><h4 id="sec4dot1dot2-entropy-25-00174title" class="inline">4.1.2. Implementation Details </h4><p class="p p-first-last">We implemented our method on the basis of the publicly available MMDetection toolbox [<a href="#B42-entropy-25-00174" rid="B42-entropy-25-00174" class=" bibr popnode">42</a>], and used dynamic R-CNN [<a href="#B43-entropy-25-00174" rid="B43-entropy-25-00174" class=" bibr popnode">43</a>] based on ResNet-50 as the baseline. We chose ResNet-50 as the backbone network because of its two advantages: (1) ResNet-50 itself had little influence on the model, which rendered the improvement effect of the proposed model more obvious. (2) It is beneficial for researchers to conduct comparisons in the experiments since it is a classical network that has been widely used. For evaluation, we used the widely used mean average precision (mAP) [<a href="#B44-entropy-25-00174" rid="B44-entropy-25-00174" class=" bibr popnode">44</a>] with an IoU threshold of 0.5. Meanwhile, we added processing time, model size, parameters, and FLOPs in order to further detail the experimental results. Processing time refers to the time from the beginning of the training process to convergence. The model size, parameters, and FLOPs can provide a reference for measuring the model complexity. In our experiments, the basic detection network was trained using stochastic gradient descent (SGD), and the initial learning rate was set to 0.002. In the data preprocessing stage, all input images were resized into 1000 &#x000d7; 600. The weight decay was 0.0001, and the momentum was 0.9. We followed the settings in MMDetection for the other hyperparameters.</p></div></div><div id="sec4dot2-entropy-25-00174" class="sec"><h3 id="sec4dot2-entropy-25-00174title">4.2. Experiments on LogoDet-3K</h3><div id="sec4dot2dot1-entropy-25-00174" class="sec sec-first"><p></p><h4 id="sec4dot2dot1-entropy-25-00174title" class="inline">4.2.1. Comparisons with State of the Art </h4><p class="p p-first">We compared the proposed LDI-Net with several other one-stage and two-stage popular baselines, as reported in <a href="/pmc/articles/PMC9857861/table/entropy-25-00174-t002/" target="table" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-t002" rid-ob="ob-entropy-25-00174-t002" co-legend-rid=""><span>Table 2</span></a>.</p><!--table ft1--><!--table-wrap mode="anchored" t5--><div class="table-wrap anchored whole_rhythm" id="entropy-25-00174-t002"><h3>Table 2</h3><!--caption a7--><div class="caption"><p>Detection results on LogoDet-3K.</p></div><div class="xtable"><table frame="hsides" rules="groups" class="rendered small default_table"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Methods</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Backbone</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP(%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Processing Time (days)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Size (KB/epoch)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Params (M)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FLOPs (G)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">One-stage:</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">FSAF [<a href="#B45-entropy-25-00174" rid="B45-entropy-25-00174" class=" bibr popnode">45</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">78.3</td><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">336,668</td><td align="center" valign="middle" rowspan="1" colspan="1">42.92</td><td align="center" valign="middle" rowspan="1" colspan="1">349.84</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ATSS [<a href="#B46-entropy-25-00174" rid="B46-entropy-25-00174" class=" bibr popnode">46</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">79.9</td><td align="center" valign="middle" rowspan="1" colspan="1">7</td><td align="center" valign="middle" rowspan="1" colspan="1">304,457</td><td align="center" valign="middle" rowspan="1" colspan="1">38.8</td><td align="center" valign="middle" rowspan="1" colspan="1">348.86</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GFL [<a href="#B47-entropy-25-00174" rid="B47-entropy-25-00174" class=" bibr popnode">47</a>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">81.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">305,590</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">38.95</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<strong>351.96</strong>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Two-stage:</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Faster R-CNN [<a href="#B5-entropy-25-00174" rid="B5-entropy-25-00174" class=" bibr popnode">5</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">83.8</td><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">442,663</td><td align="center" valign="middle" rowspan="1" colspan="1">56.49</td><td align="center" valign="middle" rowspan="1" colspan="1">222.02</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Soft-NMS [<a href="#B48-entropy-25-00174" rid="B48-entropy-25-00174" class=" bibr popnode">48</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">82.1</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">56.28</td><td align="center" valign="middle" rowspan="1" colspan="1">177.46</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PANet [<a href="#B14-entropy-25-00174" rid="B14-entropy-25-00174" class=" bibr popnode">14</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-PAFPN</td><td align="center" valign="middle" rowspan="1" colspan="1">83.1</td><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">470,332</td><td align="center" valign="middle" rowspan="1" colspan="1">60.03</td><td align="center" valign="middle" rowspan="1" colspan="1">246.8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Cascade R-CNN [<a href="#B20-entropy-25-00174" rid="B20-entropy-25-00174" class=" bibr popnode">20</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">85.6</td><td align="center" valign="middle" rowspan="1" colspan="1">8</td><td align="center" valign="middle" rowspan="1" colspan="1">611,864</td><td align="center" valign="middle" rowspan="1" colspan="1">78.15</td><td align="center" valign="middle" rowspan="1" colspan="1">243.68</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Generalized IoU [<a href="#B49-entropy-25-00174" rid="B49-entropy-25-00174" class=" bibr popnode">49</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">84.4</td><td align="center" valign="middle" rowspan="1" colspan="1">7</td><td align="center" valign="middle" rowspan="1" colspan="1">442,663</td><td align="center" valign="middle" rowspan="1" colspan="1">56.49</td><td align="center" valign="middle" rowspan="1" colspan="1">222.02</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Libra R-CNN [<a href="#B50-entropy-25-00174" rid="B50-entropy-25-00174" class=" bibr popnode">50</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-BFP</td><td align="center" valign="middle" rowspan="1" colspan="1">82.4</td><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">444,726</td><td align="center" valign="middle" rowspan="1" colspan="1">56.76</td><td align="center" valign="middle" rowspan="1" colspan="1">223.07</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Guided Anchoring [<a href="#B12-entropy-25-00174" rid="B12-entropy-25-00174" class=" bibr popnode">12</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">86.3</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">57.08</td><td align="center" valign="middle" rowspan="1" colspan="1">221.79</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Distance IoU [<a href="#B51-entropy-25-00174" rid="B51-entropy-25-00174" class=" bibr popnode">51</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">83.5</td><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">442,663</td><td align="center" valign="middle" rowspan="1" colspan="1">56.49</td><td align="center" valign="middle" rowspan="1" colspan="1">222.02</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Complete IoU [<a href="#B51-entropy-25-00174" rid="B51-entropy-25-00174" class=" bibr popnode">51</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">82.7</td><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">442,663</td><td align="center" valign="middle" rowspan="1" colspan="1">56.49</td><td align="center" valign="middle" rowspan="1" colspan="1">222.02</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Dynamic R-CNN [<a href="#B43-entropy-25-00174" rid="B43-entropy-25-00174" class=" bibr popnode">43</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">87.1</td><td align="center" valign="middle" rowspan="1" colspan="1">8</td><td align="center" valign="middle" rowspan="1" colspan="1">442,664</td><td align="center" valign="middle" rowspan="1" colspan="1">56.49</td><td align="center" valign="middle" rowspan="1" colspan="1">222.02</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SABL [<a href="#B52-entropy-25-00174" rid="B52-entropy-25-00174" class=" bibr popnode">52</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">85.7</td><td align="center" valign="middle" rowspan="1" colspan="1">6</td><td align="center" valign="middle" rowspan="1" colspan="1">352,738</td><td align="center" valign="middle" rowspan="1" colspan="1">44.98</td><td align="center" valign="middle" rowspan="1" colspan="1">269.34</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Sparse R-CNN [<a href="#B53-entropy-25-00174" rid="B53-entropy-25-00174" class=" bibr popnode">53</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">74.3</td><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">1,297,338</td><td align="center" valign="middle" rowspan="1" colspan="1">110.57</td><td align="center" valign="middle" rowspan="1" colspan="1">150.36</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<strong>LDI-Net(ours)</strong>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<strong>ResNet-50-MRNAS</strong>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<strong>88.7</strong>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<strong>10</strong>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<strong>1,443,658</strong>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<strong>183.66</strong>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">152.1</td></tr></tbody></table></div><div class="largeobj-link align_right" id="largeobj_idm139848924764240"><a target="object" rel="noopener" href="/pmc/articles/PMC9857861/table/entropy-25-00174-t002/?report=objectonly">Open in a separate window</a></div></div><p class="p p-last"><a href="/pmc/articles/PMC9857861/table/entropy-25-00174-t002/" target="table" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-t002" rid-ob="ob-entropy-25-00174-t002" co-legend-rid=""><span>Table 2</span></a> shows the best detection performance of all methods with a uniform learning rate. The proposed LDI-Net method was superior to other baselines, as it achieved the best performance with 88.7% mAP. It achieved 4.9% and 1.6% improvements over faster R-CNN and dynamic R-CNN, respectively. The proposed LDI-Net strategy also achieved the best performance compared to other approaches that utilize feature fusion. For example, PANet, Libra R-CNN, and our method all utilize feature fusion to extract multilevel features. In comparison, LDI-Net achieved 5.6% and 5.3% improvement over PANet and Libra R-CNN, respectively. To verify the detection of large aspect ratio logos, we also compared guided anchoring and achieved 2.4% accuracy improvement, which also shows our method&#x02019;s advantages in long-range interactions. In comparison with the other baselines, the proposed LDI-Net improved mAP by 10.4%, 8.8%, 7.5%, 6.6%, 4.3%, 5.2%, 6.0% 2.0% and 14.4% compared with FSAF, ATSS, GFL, Soft-NMS, generalized IoU, distance IoU, complete IoU, SABL and sparse R-CNN, respectively.</p></div><div id="sec4dot2dot2-entropy-25-00174" class="sec sec-last"><p></p><h4 id="sec4dot2dot2-entropy-25-00174title" class="inline">4.2.2. Qualitative Analysis </h4><p class="p p-first">In <a href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f005/" target="figure" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-f005" rid-ob="ob-entropy-25-00174-f005" co-legend-rid="lgnd_entropy-25-00174-f005"><span>Figure 5</span></a>, we present some illustrative examples for LDI-Net. Our model could achieve good detection performance on regular large logos and logos with large aspect ratios. For example, the detection of Warburtons and CINNZEO showed good performance on multiscale logos, while the detection of Bubbly and BOLD ROCK showed great results on deformed logos in the second line. Our model also achieved over 98% detection accuracy on Skittles and eatZis with a large proportion, and logos with disparate aspect ratios (e.g., Intusium23 and Brigham&#x02019;s) in the third line.</p><!--fig ft0--><!--fig mode=article f1--><div class="fig iconblock whole_rhythm" id="entropy-25-00174-f005" co-legend-rid="lgnd_entropy-25-00174-f005"><a href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f005/" target="figure" rid-figpopup="entropy-25-00174-f005" rid-ob="ob-entropy-25-00174-f005"><!--fig/graphic|fig/alternatives/graphic mode="anchored" m1--><div class="figure" data-largeobj="" data-largeobj-link-rid="largeobj_idm139848919289200"><a class="inline_block ts_canvas" href="/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=9857861_entropy-25-00174-g005.jpg" target="tileshopwindow" rel="noopener"><div class="ts_bar small" title="Click on image to zoom"></div><img loading="lazy" alt="An external file that holds a picture, illustration, etc.&#10;Object name is entropy-25-00174-g005.jpg" title="Click on image to zoom" class="tileshop" src="/pmc/articles/PMC9857861/bin/entropy-25-00174-g005.jpg" /></a></div><div class="largeobj-link align_right" id="largeobj_idm139848919289200"><a target="object" rel="noopener" href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f005/?report=objectonly">Open in a separate window</a></div></a><div class="icnblk_cntnt" id="lgnd_entropy-25-00174-f005"><div><a class="figpopup" href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f005/" target="figure" rid-figpopup="entropy-25-00174-f005" rid-ob="ob-entropy-25-00174-f005">Figure 5</a></div><!--caption a7--><div class="caption"><p>Some examples of LDI-Net test results. The orange box represents the location of the detected logo object. The top of the box represents categories and accuracy.</p></div></div></div><p class="p p-last">We also set up different iterations to compare the proposed strategy and dynamic R-CNN in terms of convergence and accuracy. <a href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f006/" target="figure" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-f006" rid-ob="ob-entropy-25-00174-f006" co-legend-rid="lgnd_entropy-25-00174-f006"><span>Figure 6</span></a> provides the performance trend when the iterations increased, showing that our method gradually stabilized, starting from 250,000 and converging at 350,000. During the training process, it was clear that our method maintained a higher mAP than that of dynamic R-CNN.</p><!--fig ft0--><!--fig mode=article f1--><div class="fig iconblock whole_rhythm" id="entropy-25-00174-f006" co-legend-rid="lgnd_entropy-25-00174-f006"><a href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f006/" target="figure" rid-figpopup="entropy-25-00174-f006" rid-ob="ob-entropy-25-00174-f006"><!--fig/graphic|fig/alternatives/graphic mode="anchored" m1--><div class="figure" data-largeobj="" data-largeobj-link-rid="largeobj_idm139848919286512"><a class="inline_block ts_canvas" href="/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=9857861_entropy-25-00174-g006.jpg" target="tileshopwindow" rel="noopener"><div class="ts_bar small" title="Click on image to zoom"></div><img loading="lazy" alt="An external file that holds a picture, illustration, etc.&#10;Object name is entropy-25-00174-g006.jpg" title="Click on image to zoom" class="tileshop" src="/pmc/articles/PMC9857861/bin/entropy-25-00174-g006.jpg" /></a></div><div class="largeobj-link align_right" id="largeobj_idm139848919286512"><a target="object" rel="noopener" href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f006/?report=objectonly">Open in a separate window</a></div></a><div class="icnblk_cntnt" id="lgnd_entropy-25-00174-f006"><div><a class="figpopup" href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f006/" target="figure" rid-figpopup="entropy-25-00174-f006" rid-ob="ob-entropy-25-00174-f006">Figure 6</a></div><!--caption a7--><div class="caption"><p>Comparison of dynamic R-CNN and LDI-Net with increasing number of iterations.</p></div></div></div></div></div><div id="sec4dot3-entropy-25-00174" class="sec"><h3 id="sec4dot3-entropy-25-00174title">4.3. Experiments on Other Benchmarks</h3><div id="sec4dot3dot1-entropy-25-00174" class="sec sec-first"><p></p><h4 id="sec4dot3dot1-entropy-25-00174title" class="inline">4.3.1. Results on LogoDet-3K-1000 </h4><p class="p p-first-last">LogoDet-3K-1000 is a subdataset of LogoDet-3K that has a suitable number of images and categories. Experiments on this dataset helped in further evaluating our model. We used different strategies on LogoDet-3K-1000 and list the results in <a href="/pmc/articles/PMC9857861/table/entropy-25-00174-t003/" target="table" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-t003" rid-ob="ob-entropy-25-00174-t003" co-legend-rid=""><span>Table 3</span></a>. The proposed strategy outperformed other baseline approaches and achieved 90.4% mAP. In detail, it achieved 1.3%, 2.0%, 1.3%, 1.9%, and 3.6% improvement compared with PANet, Libra R-CNN, guided anchoring, dynamic R-CNN, and sparse R-CNN, respectively.</p><!--table ft1--><!--table-wrap mode="anchored" t5--><div class="table-wrap anchored whole_rhythm" id="entropy-25-00174-t003"><h3>Table 3</h3><!--caption a7--><div class="caption"><p>Detection results on LogoDet-3K-1000.</p></div><div class="xtable"><table frame="hsides" rules="groups" class="rendered small default_table"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Methods</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Backbone</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP(%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">One-stage:</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">FSAF [<a href="#B45-entropy-25-00174" rid="B45-entropy-25-00174" class=" bibr popnode">45</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">87.3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ATSS [<a href="#B46-entropy-25-00174" rid="B46-entropy-25-00174" class=" bibr popnode">46</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">87.8</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GFL [<a href="#B47-entropy-25-00174" rid="B47-entropy-25-00174" class=" bibr popnode">47</a>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Two-stage:</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Faster R-CNN [<a href="#B5-entropy-25-00174" rid="B5-entropy-25-00174" class=" bibr popnode">5</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">88.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Soft-NMS [<a href="#B48-entropy-25-00174" rid="B48-entropy-25-00174" class=" bibr popnode">48</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">89.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PANet [<a href="#B14-entropy-25-00174" rid="B14-entropy-25-00174" class=" bibr popnode">14</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-PAFPN</td><td align="center" valign="middle" rowspan="1" colspan="1">89.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Cascade R-CNN [<a href="#B20-entropy-25-00174" rid="B20-entropy-25-00174" class=" bibr popnode">20</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">89.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Generalized IoU [<a href="#B49-entropy-25-00174" rid="B49-entropy-25-00174" class=" bibr popnode">49</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">88.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Libra R-CNN [<a href="#B50-entropy-25-00174" rid="B50-entropy-25-00174" class=" bibr popnode">50</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-BFP</td><td align="center" valign="middle" rowspan="1" colspan="1">88.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Guided anchoring [<a href="#B12-entropy-25-00174" rid="B12-entropy-25-00174" class=" bibr popnode">12</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">89.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Distance IoU [<a href="#B51-entropy-25-00174" rid="B51-entropy-25-00174" class=" bibr popnode">51</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">88.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Complete IoU [<a href="#B51-entropy-25-00174" rid="B51-entropy-25-00174" class=" bibr popnode">51</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">88.9</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Dynamic R-CNN [<a href="#B43-entropy-25-00174" rid="B43-entropy-25-00174" class=" bibr popnode">43</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">88.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SABL [<a href="#B52-entropy-25-00174" rid="B52-entropy-25-00174" class=" bibr popnode">52</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">88.8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Sparse R-CNN [<a href="#B53-entropy-25-00174" rid="B53-entropy-25-00174" class=" bibr popnode">53</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">86.8</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<strong>LDI-Net(ours)</strong>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<strong>ResNet-50-MRNAS</strong>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<strong>90.4</strong>
</td></tr></tbody></table></div><div class="largeobj-link align_right" id="largeobj_idm139848932017568"><a target="object" rel="noopener" href="/pmc/articles/PMC9857861/table/entropy-25-00174-t003/?report=objectonly">Open in a separate window</a></div></div></div><div id="sec4dot3dot2-entropy-25-00174" class="sec"><p></p><h4 id="sec4dot3dot2-entropy-25-00174title" class="inline">4.3.2. Results on QMUL-OpenLogo </h4><p class="p p-first-last">We provide experimental results on QMUL-OpenLogo to verify the effectiveness of LDI-Net. As shown in <a href="/pmc/articles/PMC9857861/table/entropy-25-00174-t004/" target="table" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-t004" rid-ob="ob-entropy-25-00174-t004" co-legend-rid=""><span>Table 4</span></a>, our model obtained 56.3% mAP, which outperformed all the other baselines. It achieved 2.5% and 14.7% improvements compared with classical algorithms faster R-CNN and SSD, respectively. Both dynamic R-CNN and Libra R-CNN achieved 54.6% mAP, our method still achieved a 1.7% improvement. Our method was also superior to feature fusion-based methods, e.g., PANet. These comparisons further verify the superiority of our method in information exchange and feature fusion.</p><!--table ft1--><!--table-wrap mode="anchored" t5--><div class="table-wrap anchored whole_rhythm" id="entropy-25-00174-t004"><h3>Table 4</h3><!--caption a7--><div class="caption"><p>Detection results on QMUL-OpenLogo.</p></div><div class="xtable"><table frame="hsides" rules="groups" class="rendered small default_table"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Methods</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Backbone</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP(%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">One-stage:</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SSD [<a href="#B6-entropy-25-00174" rid="B6-entropy-25-00174" class=" bibr popnode">6</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">VGG-16</td><td align="center" valign="middle" rowspan="1" colspan="1">41.6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">FSAF [<a href="#B45-entropy-25-00174" rid="B45-entropy-25-00174" class=" bibr popnode">45</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">44.6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ATSS [<a href="#B46-entropy-25-00174" rid="B46-entropy-25-00174" class=" bibr popnode">46</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">48.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GFL [<a href="#B47-entropy-25-00174" rid="B47-entropy-25-00174" class=" bibr popnode">47</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">47.3</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FoveaBox [<a href="#B54-entropy-25-00174" rid="B54-entropy-25-00174" class=" bibr popnode">54</a>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">35.6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Two-stage:</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Faster R-CNN [<a href="#B5-entropy-25-00174" rid="B5-entropy-25-00174" class=" bibr popnode">5</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">53.8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Soft-NMS [<a href="#B48-entropy-25-00174" rid="B48-entropy-25-00174" class=" bibr popnode">48</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">54.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PANet [<a href="#B14-entropy-25-00174" rid="B14-entropy-25-00174" class=" bibr popnode">14</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-PAFPN</td><td align="center" valign="middle" rowspan="1" colspan="1">54.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Cascade R-CNN [<a href="#B20-entropy-25-00174" rid="B20-entropy-25-00174" class=" bibr popnode">20</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">54.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Generalized IoU [<a href="#B49-entropy-25-00174" rid="B49-entropy-25-00174" class=" bibr popnode">49</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">54.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Libra R-CNN [<a href="#B50-entropy-25-00174" rid="B50-entropy-25-00174" class=" bibr popnode">50</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-BFP</td><td align="center" valign="middle" rowspan="1" colspan="1">54.6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Guided Anchoring [<a href="#B12-entropy-25-00174" rid="B12-entropy-25-00174" class=" bibr popnode">12</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">52.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Distance IoU [<a href="#B51-entropy-25-00174" rid="B51-entropy-25-00174" class=" bibr popnode">51</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">54.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Complete IoU [<a href="#B51-entropy-25-00174" rid="B51-entropy-25-00174" class=" bibr popnode">51</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">53.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Dynamic R-CNN [<a href="#B43-entropy-25-00174" rid="B43-entropy-25-00174" class=" bibr popnode">43</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">54.6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Double-head R-CNN [<a href="#B55-entropy-25-00174" rid="B55-entropy-25-00174" class=" bibr popnode">55</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">54.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SABL [<a href="#B52-entropy-25-00174" rid="B52-entropy-25-00174" class=" bibr popnode">52</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">53.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Sparse R-CNN [<a href="#B53-entropy-25-00174" rid="B53-entropy-25-00174" class=" bibr popnode">53</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">50.5</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<strong>LDI-Net(ours)</strong>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<strong>ResNet-50-MRNAS</strong>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<strong>56.3</strong>
</td></tr></tbody></table></div><div class="largeobj-link align_right" id="largeobj_idm139848926860416"><a target="object" rel="noopener" href="/pmc/articles/PMC9857861/table/entropy-25-00174-t004/?report=objectonly">Open in a separate window</a></div></div></div><div id="sec4dot3dot3-entropy-25-00174" class="sec sec-last"><p></p><h4 id="sec4dot3dot3-entropy-25-00174title" class="inline">4.3.3. Results on FlickrLogos-32 </h4><p class="p p-first-last">We also performed a comparison on FlickrLogos-32. <a href="/pmc/articles/PMC9857861/table/entropy-25-00174-t005/" target="table" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-t005" rid-ob="ob-entropy-25-00174-t005" co-legend-rid=""><span>Table 5</span></a> shows that our algorithm achieved a significant improvement compared to the base algorithms, and had the best performance with 89.8% mAP. For example, it achieved 2.7% and 1.6% mAP improvement compared with one-stage algorithm GFL and two-stage algorithm faster R-CNN, respectively. Guided anchoring is an improvement on anchor, and our method outperformed it by 1.4%. These results indicate that the proposed method is efficient in detecting logos with a large aspect ratio. Dynamic R-CNN and LDI-Net showed similar performance trends on FlickrLogos-32, as shown in <a href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f006/" target="figure" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-f006" rid-ob="ob-entropy-25-00174-f006" co-legend-rid="lgnd_entropy-25-00174-f006"><span>Figure 6</span></a>b, because of the poor quantity and quality of images in the dataset.</p><!--table ft1--><!--table-wrap mode="anchored" t5--><div class="table-wrap anchored whole_rhythm" id="entropy-25-00174-t005"><h3>Table 5</h3><!--caption a7--><div class="caption"><p>Detection results on FlickrLogos-32.</p></div><div class="xtable"><table frame="hsides" rules="groups" class="rendered small default_table"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Methods</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Backbone</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP(%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">One-stage:</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SSD [<a href="#B6-entropy-25-00174" rid="B6-entropy-25-00174" class=" bibr popnode">6</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">VGG-16</td><td align="center" valign="middle" rowspan="1" colspan="1">80.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RetinaNet [<a href="#B7-entropy-25-00174" rid="B7-entropy-25-00174" class=" bibr popnode">7</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">78.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">FSAF [<a href="#B45-entropy-25-00174" rid="B45-entropy-25-00174" class=" bibr popnode">45</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">86.3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ATSS [<a href="#B46-entropy-25-00174" rid="B46-entropy-25-00174" class=" bibr popnode">46</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">86.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GFL [<a href="#B47-entropy-25-00174" rid="B47-entropy-25-00174" class=" bibr popnode">47</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">87.2</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FoveaBox [<a href="#B54-entropy-25-00174" rid="B54-entropy-25-00174" class=" bibr popnode">54</a>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Two-stage:</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Deep Logo [<a href="#B56-entropy-25-00174" rid="B56-entropy-25-00174" class=" bibr popnode">56</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">VGG-16</td><td align="center" valign="middle" rowspan="1" colspan="1">74.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Faster R-CNN [<a href="#B5-entropy-25-00174" rid="B5-entropy-25-00174" class=" bibr popnode">5</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">88.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BD-FRCN-M [<a href="#B57-entropy-25-00174" rid="B57-entropy-25-00174" class=" bibr popnode">57</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">VGG-16</td><td align="center" valign="middle" rowspan="1" colspan="1">73.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Soft-NMS [<a href="#B48-entropy-25-00174" rid="B48-entropy-25-00174" class=" bibr popnode">48</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">88.8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PANet [<a href="#B14-entropy-25-00174" rid="B14-entropy-25-00174" class=" bibr popnode">14</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-PAFPN</td><td align="center" valign="middle" rowspan="1" colspan="1">89.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Cascade R-CNN [<a href="#B20-entropy-25-00174" rid="B20-entropy-25-00174" class=" bibr popnode">20</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">89.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Generalized IoU [<a href="#B49-entropy-25-00174" rid="B49-entropy-25-00174" class=" bibr popnode">49</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">88.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Libra R-CNN [<a href="#B50-entropy-25-00174" rid="B50-entropy-25-00174" class=" bibr popnode">50</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-BFP</td><td align="center" valign="middle" rowspan="1" colspan="1">89.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Guided Anchoring [<a href="#B12-entropy-25-00174" rid="B12-entropy-25-00174" class=" bibr popnode">12</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">88.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Distance IoU [<a href="#B51-entropy-25-00174" rid="B51-entropy-25-00174" class=" bibr popnode">51</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">88.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Complete IoU [<a href="#B51-entropy-25-00174" rid="B51-entropy-25-00174" class=" bibr popnode">51</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">89.0</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Dynamic R-CNN [<a href="#B43-entropy-25-00174" rid="B43-entropy-25-00174" class=" bibr popnode">43</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">88.9</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Double-head R-CNN [<a href="#B55-entropy-25-00174" rid="B55-entropy-25-00174" class=" bibr popnode">55</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">89.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SABL [<a href="#B52-entropy-25-00174" rid="B52-entropy-25-00174" class=" bibr popnode">52</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">88.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Sparse R-CNN [<a href="#B53-entropy-25-00174" rid="B53-entropy-25-00174" class=" bibr popnode">53</a>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50-FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">81.6</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<strong>LDI-Net(ours)</strong>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<strong>ResNet-50-MRNAS</strong>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<strong>89.8</strong>
</td></tr></tbody></table></div><div class="largeobj-link align_right" id="largeobj_idm139848922261184"><a target="object" rel="noopener" href="/pmc/articles/PMC9857861/table/entropy-25-00174-t005/?report=objectonly">Open in a separate window</a></div></div></div></div><div id="sec4dot4-entropy-25-00174" class="sec sec-last"><h3 id="sec4dot4-entropy-25-00174title">4.4. Ablation Study</h3><p class="p p-first">In this section, we conduct comprehensive analysis of the effects of each LDI-Net component on four logo datasets. We compare the test and localization accuracy of each LDI-Net component with dynamic R-CNN, namely, LD involution, MRNAS, and ARM. We used dynamic R-CNN equipped with ResNet-50 and FPN as the baseline.</p><div id="sec4dot4dot1-entropy-25-00174" class="sec"><p></p><h4 id="sec4dot4dot1-entropy-25-00174title" class="inline">4.4.1. LD Involution </h4><p class="p p-first">LD involution is a targeted solution to the large aspect ratio problem. As shown in <a href="/pmc/articles/PMC9857861/table/entropy-25-00174-t006/" target="table" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-t006" rid-ob="ob-entropy-25-00174-t006" co-legend-rid=""><span>Table 6</span></a>, LD involution achieved 87.3% mAP, outperforming other baselines on LogoDet-3K. <a href="/pmc/articles/PMC9857861/table/entropy-25-00174-t007/" target="table" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-t007" rid-ob="ob-entropy-25-00174-t007" co-legend-rid=""><span>Table 7</span></a> shows that our method outperformed the baseline by 1.2% improvement on LogoDet-3K-1000. Our method also achieved 1.5% and 0.7% improvement on QMUL-OpenLogo and FlickrLogos-32, as shown in <a href="/pmc/articles/PMC9857861/table/entropy-25-00174-t008/" target="table" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-t008" rid-ob="ob-entropy-25-00174-t008" co-legend-rid=""><span>Table 8</span></a> and <a href="/pmc/articles/PMC9857861/table/entropy-25-00174-t009/" target="table" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-t009" rid-ob="ob-entropy-25-00174-t009" co-legend-rid=""><span>Table 9</span></a>, respectively. In the comparison with involution, our method also showed superiority. In particular, on the QMUL-OpenLogo, our method achieved 1% improvement.</p><!--table ft1--><!--table-wrap mode="anchored" t5--><div class="table-wrap anchored whole_rhythm" id="entropy-25-00174-t006"><h3>Table 6</h3><!--caption a7--><div class="caption"><p>Evaluating individual components on LogoDet-3K.</p></div><div class="xtable"><table frame="hsides" rules="groups" class="rendered small default_table"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Involution</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">LD Involution</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MRNAS</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">ARM</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP(%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">87.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">87.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">87.3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">87.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" rowspan="1" colspan="1">88.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">88.2</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<strong>88.7</strong>
</td></tr></tbody></table></div><div class="largeobj-link align_right" id="largeobj_idm139848919341840"><a target="object" rel="noopener" href="/pmc/articles/PMC9857861/table/entropy-25-00174-t006/?report=objectonly">Open in a separate window</a></div></div><!--table ft1--><!--table-wrap mode="anchored" t5--><div class="table-wrap anchored whole_rhythm" id="entropy-25-00174-t007"><h3>Table 7</h3><!--caption a7--><div class="caption"><p>Evaluating individual components on LogoDet-3K-1000.</p></div><div class="xtable"><table frame="hsides" rules="groups" class="rendered small default_table"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Involution</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">LD Involution</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MRNAS</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">ARM</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP(%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">88.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">89.6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">89.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">89.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" rowspan="1" colspan="1">88.9</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">89.9</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<strong>90.4</strong>
</td></tr></tbody></table></div><div class="largeobj-link align_right" id="largeobj_idm139848932139440"><a target="object" rel="noopener" href="/pmc/articles/PMC9857861/table/entropy-25-00174-t007/?report=objectonly">Open in a separate window</a></div></div><!--table ft1--><!--table-wrap mode="anchored" t5--><div class="table-wrap anchored whole_rhythm" id="entropy-25-00174-t008"><h3>Table 8</h3><!--caption a7--><div class="caption"><p>Evaluating individual components on QMUL-OpenLogo.</p></div><div class="xtable"><table frame="hsides" rules="groups" class="rendered small default_table"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Involution</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">LD Involution</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MRNAS</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">ARM</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP(%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">54.6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">55.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">56.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">56.0</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" rowspan="1" colspan="1">54.8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">56.2</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<strong>56.3</strong>
</td></tr></tbody></table></div><div class="largeobj-link align_right" id="largeobj_idm139848917186464"><a target="object" rel="noopener" href="/pmc/articles/PMC9857861/table/entropy-25-00174-t008/?report=objectonly">Open in a separate window</a></div></div><!--table ft1--><!--table-wrap mode="anchored" t5--><div class="table-wrap anchored whole_rhythm" id="entropy-25-00174-t009"><h3>Table 9</h3><!--caption a7--><div class="caption"><p>Evaluating individual components on FlickrLogos-32.</p></div><div class="xtable"><table frame="hsides" rules="groups" class="rendered small default_table"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Involution</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">LD Involution</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MRNAS</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">ARM</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP(%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">88.9</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">89.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">89.6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">89.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" rowspan="1" colspan="1">89.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">89.8</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<strong>89.8</strong>
</td></tr></tbody></table></div><div class="largeobj-link align_right" id="largeobj_idm139848933921424"><a target="object" rel="noopener" href="/pmc/articles/PMC9857861/table/entropy-25-00174-t009/?report=objectonly">Open in a separate window</a></div></div><p class="p p-last"><a href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f007/" target="figure" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-f007" rid-ob="ob-entropy-25-00174-f007" co-legend-rid="lgnd_entropy-25-00174-f007"><span>Figure 7</span></a> shows the visualization comparison results of dynamic R-CNN and LD involution on LogoDet-3K. A logo image with a large aspect ratio is taken as the visual displaying result, which shows that the accuracy of our method was higher than the baseline. For example, for the logo with a very wide aspect ratio, our model achieved 3% and 27% improvement over the baseline, as shown in <a href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f007/" target="figure" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-f007" rid-ob="ob-entropy-25-00174-f007" co-legend-rid="lgnd_entropy-25-00174-f007"><span>Figure 7</span></a>a,b, respectively. In addition, LD involution could identify small logo &#x02018;SEADOO&#x02019;, while the baseline could not, as shown in <a href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f007/" target="figure" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-f007" rid-ob="ob-entropy-25-00174-f007" co-legend-rid="lgnd_entropy-25-00174-f007"><span>Figure 7</span></a>b. This indicates that LD involution extracting long-range information is also effective for small objects with a large aspect ratio.</p><!--fig ft0--><!--fig mode=article f1--><div class="fig iconblock whole_rhythm" id="entropy-25-00174-f007" co-legend-rid="lgnd_entropy-25-00174-f007"><a href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f007/" target="figure" rid-figpopup="entropy-25-00174-f007" rid-ob="ob-entropy-25-00174-f007"><!--fig/graphic|fig/alternatives/graphic mode="anchored" m1--><div class="figure" data-largeobj="" data-largeobj-link-rid="largeobj_idm139848924648208"><a class="inline_block ts_canvas" href="/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=9857861_entropy-25-00174-g007.jpg" target="tileshopwindow" rel="noopener"><div class="ts_bar small" title="Click on image to zoom"></div><img loading="lazy" alt="An external file that holds a picture, illustration, etc.&#10;Object name is entropy-25-00174-g007.jpg" title="Click on image to zoom" class="tileshop" src="/pmc/articles/PMC9857861/bin/entropy-25-00174-g007.jpg" /></a></div><div class="largeobj-link align_right" id="largeobj_idm139848924648208"><a target="object" rel="noopener" href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f007/?report=objectonly">Open in a separate window</a></div></a><div class="icnblk_cntnt" id="lgnd_entropy-25-00174-f007"><div><a class="figpopup" href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f007/" target="figure" rid-figpopup="entropy-25-00174-f007" rid-ob="ob-entropy-25-00174-f007">Figure 7</a></div><!--caption a7--><div class="caption"><p>Comparison of visualization results of dynamic R-CNN and LDI-Net for the large aspect ratio problem. Blue boxes: ground-truth boxes. Orange boxes: correct detection boxes.</p></div></div></div></div><div id="sec4dot4dot2-entropy-25-00174" class="sec"><p></p><h4 id="sec4dot4dot2-entropy-25-00174title" class="inline">4.4.2. MRNAS </h4><p class="p p-first">We applied the MRNAS module to solve multiscale problems and achieved good results. The MRNAS module performed well on two large logo detection datasets, i.e., LogoDet-3K and LogoDet-3K-1000. As shown in <a href="/pmc/articles/PMC9857861/table/entropy-25-00174-t006/" target="table" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-t006" rid-ob="ob-entropy-25-00174-t006" co-legend-rid=""><span>Table 6</span></a>, on LogoDet-3K, the mAP of our model with MRNAS reached 87.5%, a 0.4% improvement over dynamic R-CNN. On LogoDet-3K-1000, the model with MRNAS reached 89.5% mAP, achieving 1% improvement over the baseline, as seen in <a href="/pmc/articles/PMC9857861/table/entropy-25-00174-t007/" target="table" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-t007" rid-ob="ob-entropy-25-00174-t007" co-legend-rid=""><span>Table 7</span></a>. In addition, MRNAS achieved good performance on the other two datasets, in which mAP was significantly improved, as can be seen in <a href="/pmc/articles/PMC9857861/table/entropy-25-00174-t008/" target="table" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-t008" rid-ob="ob-entropy-25-00174-t008" co-legend-rid=""><span>Table 8</span></a> and <a href="/pmc/articles/PMC9857861/table/entropy-25-00174-t009/" target="table" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-t009" rid-ob="ob-entropy-25-00174-t009" co-legend-rid=""><span>Table 9</span></a>.</p><p class="p p-last">We provide some illustrative examples of logos with different scales from LogoDet-3K, as shown in <a href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f008/" target="figure" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-f008" rid-ob="ob-entropy-25-00174-f008" co-legend-rid="lgnd_entropy-25-00174-f008"><span>Figure 8</span></a>. In the first pair, the baseline could not detect the rightmost logo. In contrast, our method had a detection accuracy of 92%. Meanwhile, LDI-Net improved the detection mAP from 44% to 98% compared with dynamic R-CNN, which also shows the superiority of the proposed method in the second pair.</p><!--fig ft0--><!--fig mode=article f1--><div class="fig iconblock whole_rhythm" id="entropy-25-00174-f008" co-legend-rid="lgnd_entropy-25-00174-f008"><a href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f008/" target="figure" rid-figpopup="entropy-25-00174-f008" rid-ob="ob-entropy-25-00174-f008"><!--fig/graphic|fig/alternatives/graphic mode="anchored" m1--><div class="figure" data-largeobj="" data-largeobj-link-rid="largeobj_idm139848924645440"><a class="inline_block ts_canvas" href="/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=9857861_entropy-25-00174-g008.jpg" target="tileshopwindow" rel="noopener"><div class="ts_bar small" title="Click on image to zoom"></div><img loading="lazy" alt="An external file that holds a picture, illustration, etc.&#10;Object name is entropy-25-00174-g008.jpg" title="Click on image to zoom" class="tileshop" src="/pmc/articles/PMC9857861/bin/entropy-25-00174-g008.jpg" /></a></div><div class="largeobj-link align_right" id="largeobj_idm139848924645440"><a target="object" rel="noopener" href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f008/?report=objectonly">Open in a separate window</a></div></a><div class="icnblk_cntnt" id="lgnd_entropy-25-00174-f008"><div><a class="figpopup" href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f008/" target="figure" rid-figpopup="entropy-25-00174-f008" rid-ob="ob-entropy-25-00174-f008">Figure 8</a></div><!--caption a7--><div class="caption"><p>Comparison of visualization results of dynamic R-CNN and LDI-Net for multiscale logo images. Blue boxes: ground-truth boxes. Orange boxes: correct detection boxes.</p></div></div></div></div><div id="sec4dot4dot3-entropy-25-00174" class="sec sec-last"><p></p><h4 id="sec4dot4dot3-entropy-25-00174title" class="inline">4.4.3. ARM </h4><p class="p p-first">We conducted ablation experiments for ARM on four datasets, and the experimental results demonstrate that ARM works better than the baselines. As shown in <a href="/pmc/articles/PMC9857861/table/entropy-25-00174-t006/" target="table" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-t006" rid-ob="ob-entropy-25-00174-t006" co-legend-rid=""><span>Table 6</span></a>, the performance of a single ARM module was comparable to that of two other modules, up to 88.2% on LogoDet-3K. In the other three datasets (<a href="/pmc/articles/PMC9857861/table/entropy-25-00174-t007/" target="table" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-t007" rid-ob="ob-entropy-25-00174-t007" co-legend-rid=""><span>Table 7</span></a>, <a href="/pmc/articles/PMC9857861/table/entropy-25-00174-t008/" target="table" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-t008" rid-ob="ob-entropy-25-00174-t008" co-legend-rid=""><span>Table 8</span></a> and <a href="/pmc/articles/PMC9857861/table/entropy-25-00174-t009/" target="table" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-t009" rid-ob="ob-entropy-25-00174-t009" co-legend-rid=""><span>Table 9</span></a>), ARM also achieved an improvement in mAP. The results indicate that ARM can be an effective solution to logo deformation.</p><p>We selected a variety of deformed logos for different reasons to fully illustrate the functionality of ARM in a visualization experiment. <a href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f009/" target="figure" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-f009" rid-ob="ob-entropy-25-00174-f009" co-legend-rid="lgnd_entropy-25-00174-f009"><span>Figure 9</span></a>a shows that our model could still achieve 82% detection accuracy on the logo that was deformed due to the camera angle. As shown in <a href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f009/" target="figure" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-f009" rid-ob="ob-entropy-25-00174-f009" co-legend-rid="lgnd_entropy-25-00174-f009"><span>Figure 9</span></a>b, our model could detect incomplete logo &#x02018;TIMEX&#x02019;, which confirms the effectiveness of our model.</p><!--fig ft0--><!--fig mode=article f1--><div class="fig iconblock whole_rhythm" id="entropy-25-00174-f009" co-legend-rid="lgnd_entropy-25-00174-f009"><a href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f009/" target="figure" rid-figpopup="entropy-25-00174-f009" rid-ob="ob-entropy-25-00174-f009"><!--fig/graphic|fig/alternatives/graphic mode="anchored" m1--><div class="figure" data-largeobj="" data-largeobj-link-rid="largeobj_idm139848924642672"><a class="inline_block ts_canvas" href="/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=9857861_entropy-25-00174-g009.jpg" target="tileshopwindow" rel="noopener"><div class="ts_bar small" title="Click on image to zoom"></div><img loading="lazy" alt="An external file that holds a picture, illustration, etc.&#10;Object name is entropy-25-00174-g009.jpg" title="Click on image to zoom" class="tileshop" src="/pmc/articles/PMC9857861/bin/entropy-25-00174-g009.jpg" /></a></div><div class="largeobj-link align_right" id="largeobj_idm139848924642672"><a target="object" rel="noopener" href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f009/?report=objectonly">Open in a separate window</a></div></a><div class="icnblk_cntnt" id="lgnd_entropy-25-00174-f009"><div><a class="figpopup" href="/pmc/articles/PMC9857861/figure/entropy-25-00174-f009/" target="figure" rid-figpopup="entropy-25-00174-f009" rid-ob="ob-entropy-25-00174-f009">Figure 9</a></div><!--caption a7--><div class="caption"><p>Comparison of visualization results of dynamic R-CNN and LDI-Net for logo deformation images. Blue boxes: ground-truth boxes. Orange boxes: correct detection boxes.</p></div></div></div><p class="p p-last">After testing the components individually, we conducted experiments combining LD involution and MRNAS to further validate the model effects, as shown in <a href="/pmc/articles/PMC9857861/table/entropy-25-00174-t006/" target="table" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-t006" rid-ob="ob-entropy-25-00174-t006" co-legend-rid=""><span>Table 6</span></a>, <a href="/pmc/articles/PMC9857861/table/entropy-25-00174-t007/" target="table" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-t007" rid-ob="ob-entropy-25-00174-t007" co-legend-rid=""><span>Table 7</span></a>, <a href="/pmc/articles/PMC9857861/table/entropy-25-00174-t008/" target="table" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-t008" rid-ob="ob-entropy-25-00174-t008" co-legend-rid=""><span>Table 8</span></a> and <a href="/pmc/articles/PMC9857861/table/entropy-25-00174-t009/" target="table" class="fig-table-link figpopup" rid-figpopup="entropy-25-00174-t009" rid-ob="ob-entropy-25-00174-t009" co-legend-rid=""><span>Table 9</span></a>. On all four datasets, the combination of LD involution and MRNAS performed better than adding only one module.</p></div></div></div><div id="sec5-entropy-25-00174" class="tsec sec"><h2 class="head no_bottom_margin" id="sec5-entropy-25-00174title">5. Conclusions and Future Work</h2><p class="p p-first">In this paper, we proposed a logo detection model, long-range dependence involutional network (LDI-Net), to detect logos with large aspect ratios by adding a new operator and a self-attention mechanism. Meanwhile, MRNAS was proposed to construct a novel multipath topology to realize multiscale logo detection. ARM was also introduced to enhance the ability of the proposed model to handle logo deformation.</p><p class="p p-last">So far, LDI-Net has worked well, but there are some limitations. Although multiscale logos can be completed well, there is still room for further improvement in the localization and classification of some small logos. Our method could also solve the problem of logo deformation caused by occlusion and rotation very well, but the deformations caused by reflection and distortion need to be studied more specifically. In future work, we will continue to conduct indepth research to solve the above problems. Further, we will address other challenges of logo detection, such as small, similar, and low-resolution logos.</p></div><div id="funding-group-a.ad.b.r" class="tsec sec"><h2 class="head no_bottom_margin" id="funding-group-a.ad.b.rtitle">Funding Statement</h2><p>This research was funded by the National Nature Science Foundation of China (nos. 62072289 and 62003196) and the Natural Science Foundation of Shandong Province in China (ZR2020MF076).</p></div><div id="notes-a.af.b" class="tsec sec"><h2 class="head no_bottom_margin" id="notes-a.af.btitle">Author Contributions</h2><p>Conceptualization, S.H. and X.L.; methodology, S.H. and X.L.; software, X.L.; validation, X.L. and B.Z.; formal analysis, X.L. and B.Z.; investigation, X.L.; resources, S.H. and Y.Z.; data curation, X.L.; writing&#x02014;original draft preparation, X.L.; writing&#x02014;review and editing, S.H., J.W. and W.J.; visualization, J.W.; supervision, Y.Z.; project administration, S.H.; funding acquisition, S.H. and Y.Z. All authors have read and agreed to the published version of the manuscript.</p></div><div id="notes-a.af.c" class="tsec sec"><h2 class="head no_bottom_margin" id="notes-a.af.ctitle">Institutional Review Board Statement</h2><p>Not applicable.</p></div><div id="notes-a.af.d" class="tsec sec"><h2 class="head no_bottom_margin" id="notes-a.af.dtitle">Informed Consent Statement</h2><p>Not applicable.</p></div><div id="notes-a.af.e" class="tsec sec"><h2 class="head no_bottom_margin" id="notes-a.af.etitle">Data Availability Statement</h2><p>The data are contained within the article.</p></div><div id="notes-a.af.f" class="tsec sec"><h2 class="head no_bottom_margin" id="notes-a.af.ftitle">Conflicts of Interest</h2><p>The authors declare no conflict of interest.</p></div><div id="fn-group-a.af.a" class="tsec sec"><h2 class="head no_bottom_margin" id="fn-group-a.af.atitle">Footnotes</h2><!--back/fn-group--><div class="fm-sec half_rhythm small"><p class="fn sec" id="fn-a.af.a.a"><p class="p p-first-last"><strong>Disclaimer/Publisher&#x02019;s Note:</strong> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></p></div></div><div id="ref-list-a.af.g" class="tsec sec"><h2 class="head no_bottom_margin" id="ref-list-a.af.gtitle">References</h2><div class="ref-list-sec sec" id="reference-list"><div class="ref-cit-blk half_rhythm" id="B1-entropy-25-00174">1. <span class="element-citation">Yang L., Luo P., Change Loy C., Tang X. A large-scale car dataset for fine-grained categorization and verification; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition; Boston, MA, USA. 7&#x02013;12 June 2015; pp. 3973–3981. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+IEEE/CVF+Conference+on+Computer+Vision+and+Pattern+Recognition&amp;title=A+large-scale+car+dataset+for+fine-grained+categorization+and+verification&amp;author=L.+Yang&amp;author=P.+Luo&amp;author=C.+Change+Loy&amp;author=X.+Tang&amp;pages=3973-3981&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B2-entropy-25-00174">2. <span class="element-citation">Ke X., Du P. Vehicle logo recognition with small sample problem in complex scene based on data augmentation. <span><span class="ref-journal">Math. Probl. Eng. </span>2020;<span class="ref-vol">2020</span>:6591873. doi: 10.1155/2020/6591873.</span> [<a href="//doi.org/10.1155%2F2020%2F6591873" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Math.+Probl.+Eng.&amp;title=Vehicle+logo+recognition+with+small+sample+problem+in+complex+scene+based+on+data+augmentation&amp;author=X.+Ke&amp;author=P.+Du&amp;volume=2020&amp;publication_year=2020&amp;pages=6591873&amp;doi=10.1155/2020/6591873&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B3-entropy-25-00174">3. <span class="element-citation">Gao Y., Wang F., Luan H., Chua T.S. Brand data gathering from live social media streams; Proceedings of the International Conference on Multimedia Retrieval; Glasgow, UK. 1&#x02013;4 April 2014; pp. 169–176. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+International+Conference+on+Multimedia+Retrieval&amp;title=Brand+data+gathering+from+live+social+media+streams&amp;author=Y.+Gao&amp;author=F.+Wang&amp;author=H.+Luan&amp;author=T.S.+Chua&amp;pages=169-176&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B4-entropy-25-00174">4. <span class="element-citation">Zhu G., Doermann D. Automatic document logo detection; Proceedings of the Ninth International Conference on Document Analysis and Recognition; Glasgow, UK. 1&#x02013;4 April 2014; pp. 864–868. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+Ninth+International+Conference+on+Document+Analysis+and+Recognition&amp;title=Automatic+document+logo+detection&amp;author=G.+Zhu&amp;author=D.+Doermann&amp;pages=864-868&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B5-entropy-25-00174">5. <span class="element-citation">Ren S., He K., Girshick R., Sun J. Faster r-cnn: Towards real-time object detection with region proposal networks. <span><span class="ref-journal">Adv. Neural Inf. Process. Syst. </span>2015;<span class="ref-vol">28</span> doi: 10.1109/TPAMI.2016.2577031.</span> [<a href="https://pubmed.ncbi.nlm.nih.gov/27295650" ref="reftype=pubmed&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord">PubMed</a>] [<a href="//doi.org/10.1109%2FTPAMI.2016.2577031" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Adv.+Neural+Inf.+Process.+Syst.&amp;title=Faster+r-cnn:+Towards+real-time+object+detection+with+region+proposal+networks&amp;author=S.+Ren&amp;author=K.+He&amp;author=R.+Girshick&amp;author=J.+Sun&amp;volume=28&amp;publication_year=2015&amp;pmid=27295650&amp;doi=10.1109/TPAMI.2016.2577031&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B6-entropy-25-00174">6. <span class="element-citation">Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.Y., Berg A.C. Ssd: Single shot multibox detector; Proceedings of the European Conference on Computer Vision; Amsterdam, The Netherlands. 11&#x02013;14 October 2016; Berlin/Heidelberg, Germany: Springer; 2016. pp. 21–37. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+European+Conference+on+Computer+Vision&amp;title=Ssd:+Single+shot+multibox+detector&amp;author=W.+Liu&amp;author=D.+Anguelov&amp;author=D.+Erhan&amp;author=C.+Szegedy&amp;author=S.+Reed&amp;publication_year=2016&amp;pages=21-37&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B7-entropy-25-00174">7. <span class="element-citation">Lin T.Y., Goyal P., Girshick R., He K., Doll&#x000e1;r P. Focal loss for dense object detection; Proceedings of the IEEE International Conference on Computer Vision; Venice, Italy. 22&#x02013;29 October 2017; pp. 2980–2988. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+IEEE+International+Conference+on+Computer+Vision&amp;title=Focal+loss+for+dense+object+detection&amp;author=T.Y.+Lin&amp;author=P.+Goyal&amp;author=R.+Girshick&amp;author=K.+He&amp;author=P.+Doll&#x000e1;r&amp;pages=2980-2988&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B8-entropy-25-00174">8. <span class="element-citation">Redmon J., Farhadi A. Yolov3: An incremental improvement. <span><span class="ref-journal">arXiv. </span>2018</span>1804.02767 <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=arXiv&amp;title=Yolov3:+An+incremental+improvement&amp;author=J.+Redmon&amp;author=A.+Farhadi&amp;publication_year=2018&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B9-entropy-25-00174">9. <span class="element-citation">Zhong Y., Wang J., Peng J., Zhang L. Anchor box optimization for object detection; Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision; Venice, Italy. 22&#x02013;29 October 2017; pp. 1286–1294. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+IEEE/CVF+Winter+Conference+on+Applications+of+Computer+Vision&amp;title=Anchor+box+optimization+for+object+detection&amp;author=Y.+Zhong&amp;author=J.+Wang&amp;author=J.+Peng&amp;author=L.+Zhang&amp;pages=1286-1294&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B10-entropy-25-00174">10. <span class="element-citation">Yang T., Zhang X., Li Z., Zhang W., Sun J. Metaanchor: Learning to detect objects with customized anchors. <span><span class="ref-journal">Adv. Neural Inf. Process. Syst. </span>2018;<span class="ref-vol">31</span>:318–328.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Adv.+Neural+Inf.+Process.+Syst.&amp;title=Metaanchor:+Learning+to+detect+objects+with+customized+anchors&amp;author=T.+Yang&amp;author=X.+Zhang&amp;author=Z.+Li&amp;author=W.+Zhang&amp;author=J.+Sun&amp;volume=31&amp;publication_year=2018&amp;pages=318-328&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B11-entropy-25-00174">11. <span class="element-citation">Kong T., Sun F., Liu H., Jiang Y., Shi J. Consistent optimization for single-shot object detection. <span><span class="ref-journal">arXiv. </span>2019</span>1901.06563 <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=arXiv&amp;title=Consistent+optimization+for+single-shot+object+detection&amp;author=T.+Kong&amp;author=F.+Sun&amp;author=H.+Liu&amp;author=Y.+Jiang&amp;author=J.+Shi&amp;publication_year=2019&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B12-entropy-25-00174">12. <span class="element-citation">Wang J., Chen K., Yang S., Loy C.C., Lin D. Region proposal by guided anchoring; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition; Long Beach, CA, USA. 16&#x02013;17 June 2019; pp. 2965–2974. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+IEEE/CVF+Conference+on+Computer+Vision+and+Pattern+Recognition&amp;title=Region+proposal+by+guided+anchoring&amp;author=J.+Wang&amp;author=K.+Chen&amp;author=S.+Yang&amp;author=C.C.+Loy&amp;author=D.+Lin&amp;pages=2965-2974&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B13-entropy-25-00174">13. <span class="element-citation">Lin T.Y., Doll&#x000e1;r P., Girshick R., He K., Hariharan B., Belongie S. Feature pyramid networks for object detection; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition; Honolulu, HI, USA. 21&#x02013;26 July 2017; pp. 2117–2125. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+IEEE/CVF+Conference+on+Computer+Vision+and+Pattern+Recognition&amp;title=Feature+pyramid+networks+for+object+detection&amp;author=T.Y.+Lin&amp;author=P.+Doll&#x000e1;r&amp;author=R.+Girshick&amp;author=K.+He&amp;author=B.+Hariharan&amp;pages=2117-2125&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B14-entropy-25-00174">14. <span class="element-citation">Liu S., Qi L., Qin H., Shi J., Jia J. Path aggregation network for instance segmentation; Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City; UT, USA. 18&#x02013;23 June 2018; pp. 8759–8768. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+2018+IEEE/CVF+Conference+on+Computer+Vision+and+Pattern+Recognition,+Salt+Lake+City&amp;title=Path+aggregation+network+for+instance+segmentation&amp;author=S.+Liu&amp;author=L.+Qi&amp;author=H.+Qin&amp;author=J.+Shi&amp;author=J.+Jia&amp;pages=8759-8768&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B15-entropy-25-00174">15. <span class="element-citation">Wang X., Zhang S., Yu Z., Feng L., Zhang W. Scale-equalizing pyramid convolution for object detection; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition; Seattle, WA, USA. 13&#x02013;19 June 2020; pp. 13359–13368. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+IEEE/CVF+Conference+on+Computer+Vision+and+Pattern+Recognition&amp;title=Scale-equalizing+pyramid+convolution+for+object+detection&amp;author=X.+Wang&amp;author=S.+Zhang&amp;author=Z.+Yu&amp;author=L.+Feng&amp;author=W.+Zhang&amp;pages=13359-13368&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B16-entropy-25-00174">16. <span class="element-citation">Dewi C., Chen R.C., Zhuang Y.C., Christanto H.J. Yolov5 Series Algorithm for Road Marking Sign Identification. <span><span class="ref-journal">Big Data Cogn. Comput. </span>2022;<span class="ref-vol">6</span>:149.  doi: 10.3390/bdcc6040149.</span> [<a href="//doi.org/10.3390%2Fbdcc6040149" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Big+Data+Cogn.+Comput.&amp;title=Yolov5+Series+Algorithm+for+Road+Marking+Sign+Identification&amp;author=C.+Dewi&amp;author=R.C.+Chen&amp;author=Y.C.+Zhuang&amp;author=H.J.+Christanto&amp;volume=6&amp;publication_year=2022&amp;pages=149&amp;doi=10.3390/bdcc6040149&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B17-entropy-25-00174">17. <span class="element-citation">El Morabit S., Rivenq A., Zighem M.E.n., Hadid A., Ouahabi A., Taleb-Ahmed A. Automatic pain estimation from facial expressions: A comparative analysis using off-the-shelf CNN architectures. <span><span class="ref-journal">Electronics. </span>2021;<span class="ref-vol">10</span>:1926.  doi: 10.3390/electronics10161926.</span> [<a href="//doi.org/10.3390%2Felectronics10161926" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Electronics&amp;title=Automatic+pain+estimation+from+facial+expressions:+A+comparative+analysis+using+off-the-shelf+CNN+architectures&amp;author=S.+El+Morabit&amp;author=A.+Rivenq&amp;author=M.E.n.+Zighem&amp;author=A.+Hadid&amp;author=A.+Ouahabi&amp;volume=10&amp;publication_year=2021&amp;pages=1926&amp;doi=10.3390/electronics10161926&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B18-entropy-25-00174">18. <span class="element-citation">Chen W., Gao L., Li X., Shen W. Lightweight convolutional neural network with knowledge distillation for cervical cells classification. <span><span class="ref-journal">Biomed. Signal Process. Control. </span>2022;<span class="ref-vol">71</span>:103177. doi: 10.1016/j.bspc.2021.103177.</span> [<a href="//doi.org/10.1016%2Fj.bspc.2021.103177" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Biomed.+Signal+Process.+Control&amp;title=Lightweight+convolutional+neural+network+with+knowledge+distillation+for+cervical+cells+classification&amp;author=W.+Chen&amp;author=L.+Gao&amp;author=X.+Li&amp;author=W.+Shen&amp;volume=71&amp;publication_year=2022&amp;pages=103177&amp;doi=10.1016/j.bspc.2021.103177&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B19-entropy-25-00174">19. <span class="element-citation">Girshick R. Fast r-cnn; Proceedings of the IEEE International Conference on Computer Vision; Santiago, Chile. 7&#x02013;13 December 2015; pp. 1440–1448. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+IEEE+International+Conference+on+Computer+Vision&amp;title=Fast+r-cnn&amp;author=R.+Girshick&amp;pages=1440-1448&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B20-entropy-25-00174">20. <span class="element-citation">Cai Z., Vasconcelos N. Cascade r-cnn: Delving into high quality object detection; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition; Salt Lake City, UT, USA. 18&#x02013;23 June 2018; pp. 6154–6162. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+IEEE/CVF+Conference+on+Computer+Vision+and+Pattern+Recognition&amp;title=Cascade+r-cnn:+Delving+into+high+quality+object+detection&amp;author=Z.+Cai&amp;author=N.+Vasconcelos&amp;pages=6154-6162&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B21-entropy-25-00174">21. <span class="element-citation">Redmon J., Divvala S., Girshick R., Farhadi A. You only look once: Unified, real-time object detection; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition; Honolulu, HI, USA. 21&#x02013;26 July 2016; pp. 779–788. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+IEEE/CVF+Conference+on+Computer+Vision+and+Pattern+Recognition&amp;title=You+only+look+once:+Unified,+real-time+object+detection&amp;author=J.+Redmon&amp;author=S.+Divvala&amp;author=R.+Girshick&amp;author=A.+Farhadi&amp;pages=779-788&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B22-entropy-25-00174">22. <span class="element-citation">Redmon J., Farhadi A. YOLO9000: Better, faster, stronger; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition; Honolulu, HI, USA. 21&#x02013;26 July 2017; pp. 7263–7271. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+IEEE/CVF+Conference+on+Computer+Vision+and+Pattern+Recognition&amp;title=YOLO9000:+Better,+faster,+stronger&amp;author=J.+Redmon&amp;author=A.+Farhadi&amp;pages=7263-7271&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B23-entropy-25-00174">23. <span class="element-citation">Tan M., Pang R., Le Q.V. Efficientdet: Scalable and efficient object detection; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition; Seattle, WA, USA. 13&#x02013;19 June 2020; pp. 10781–10790. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+IEEE/CVF+Conference+on+Computer+Vision+and+Pattern+Recognition&amp;title=Efficientdet:+Scalable+and+efficient+object+detection&amp;author=M.+Tan&amp;author=R.+Pang&amp;author=Q.V.+Le&amp;pages=10781-10790&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B24-entropy-25-00174">24. <span class="element-citation">Hou S., Li J., Min W., Hou Q., Zhao Y., Zheng Y., Jiang S. Deep Learning for Logo Detection: A Survey. <span><span class="ref-journal">arXiv. </span>2022</span>2210.04399 <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=arXiv&amp;title=Deep+Learning+for+Logo+Detection:+A+Survey&amp;author=S.+Hou&amp;author=J.+Li&amp;author=W.+Min&amp;author=Q.+Hou&amp;author=Y.+Zhao&amp;publication_year=2022&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B25-entropy-25-00174">25. <span class="element-citation">Wang J., Min W., Hou S., Ma S., Zheng Y., Jiang S. LogoDet-3K: A Large-Scale Image Dataset for Logo Detection. <span><span class="ref-journal">ACM Trans. Multimed. Comput. Commun. Appl. </span>2022;<span class="ref-vol">18</span>:1–19. doi: 10.1145/3466780.</span> [<a href="//doi.org/10.1145%2F3466780" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=ACM+Trans.+Multimed.+Comput.+Commun.+Appl.&amp;title=LogoDet-3K:+A+Large-Scale+Image+Dataset+for+Logo+Detection&amp;author=J.+Wang&amp;author=W.+Min&amp;author=S.+Hou&amp;author=S.+Ma&amp;author=Y.+Zheng&amp;volume=18&amp;publication_year=2022&amp;pages=1-19&amp;doi=10.1145/3466780&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B26-entropy-25-00174">26. <span class="element-citation">Hou Q., Min W., Wang J., Hou S., Zheng Y., Jiang S. FoodLogoDet-1500: A Dataset for Large-Scale Food Logo Detection via Multi-Scale Feature Decoupling Network; Proceedings of the 29th ACM International Conference on Multimedia; Chengdu, China. 20 October 2021; pp. 4670–4679. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+29th+ACM+International+Conference+on+Multimedia&amp;title=FoodLogoDet-1500:+A+Dataset+for+Large-Scale+Food+Logo+Detection+via+Multi-Scale+Feature+Decoupling+Network&amp;author=Q.+Hou&amp;author=W.+Min&amp;author=J.+Wang&amp;author=S.+Hou&amp;author=Y.+Zheng&amp;pages=4670-4679&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B27-entropy-25-00174">27. <span class="element-citation">Xu W., Liu Y., Lin D. A Simple and Effective Baseline for Robust Logo Detection; Proceedings of the 29th ACM International Conference on Multimedia; Nice, France. 21&#x02013;25 October 2021; pp. 4784–4788. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+29th+ACM+International+Conference+on+Multimedia&amp;title=A+Simple+and+Effective+Baseline+for+Robust+Logo+Detection&amp;author=W.+Xu&amp;author=Y.+Liu&amp;author=D.+Lin&amp;pages=4784-4788&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B28-entropy-25-00174">28. <span class="element-citation">Viola P., Jones M. Rapid object detection using a boosted cascade of simple features; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition; Kauai, HI, USA. 8&#x02013;14 December 2001; pp. I&#x02013;I. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+IEEE/CVF+Conference+on+Computer+Vision+and+Pattern+Recognition&amp;title=Rapid+object+detection+using+a+boosted+cascade+of+simple+features&amp;author=P.+Viola&amp;author=M.+Jones&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B29-entropy-25-00174">29. <span class="element-citation">Dalal N., Triggs B. Histograms of oriented gradients for human detection; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition; San Diego, CA, USA. 20&#x02013;26 June 2005; pp. 886–893. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+IEEE/CVF+Conference+on+Computer+Vision+and+Pattern+Recognition&amp;title=Histograms+of+oriented+gradients+for+human+detection&amp;author=N.+Dalal&amp;author=B.+Triggs&amp;pages=886-893&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B30-entropy-25-00174">30. <span class="element-citation">Felzenszwalb P., McAllester D., Ramanan D. A discriminatively trained, multiscale, deformable part model; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition; Anchorage, Alaska. 23&#x02013;28 June 2008; pp. 1–8. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+IEEE/CVF+Conference+on+Computer+Vision+and+Pattern+Recognition&amp;title=A+discriminatively+trained,+multiscale,+deformable+part+model&amp;author=P.+Felzenszwalb&amp;author=D.+McAllester&amp;author=D.+Ramanan&amp;pages=1-8&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B31-entropy-25-00174">31. <span class="element-citation">Yan W.Q., Wang J., Kankanhalli M.S. Automatic video logo detection and removal. <span><span class="ref-journal">Multimed. Syst. </span>2005;<span class="ref-vol">10</span>:379–391. doi: 10.1007/s00530-005-0167-6.</span> [<a href="//doi.org/10.1007%2Fs00530-005-0167-6" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Multimed.+Syst.&amp;title=Automatic+video+logo+detection+and+removal&amp;author=W.Q.+Yan&amp;author=J.+Wang&amp;author=M.S.+Kankanhalli&amp;volume=10&amp;publication_year=2005&amp;pages=379-391&amp;doi=10.1007/s00530-005-0167-6&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B32-entropy-25-00174">32. <span class="element-citation">Wang Y., Liu Z., Xiao F. A fast coarse-to-fine vehicle logo detection and recognition method; Proceedings of the 2007 IEEE International Conference on Robotics and Biomimetics; Sanya, China. 15&#x02013;18 December 2007; pp. 691–696. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+2007+IEEE+International+Conference+on+Robotics+and+Biomimetics&amp;title=A+fast+coarse-to-fine+vehicle+logo+detection+and+recognition+method&amp;author=Y.+Wang&amp;author=Z.+Liu&amp;author=F.+Xiao&amp;pages=691-696&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B33-entropy-25-00174">33. <span class="element-citation">Bao Y., Li H., Fan X., Liu R., Jia Q. Region-based CNN for logo detection; Proceedings of the International Conference on Internet Multimedia Computing and Service; Xi&#x02019;an, China. 19&#x02013;21 August 2016; pp. 319–322. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+International+Conference+on+Internet+Multimedia+Computing+and+Service&amp;title=Region-based+CNN+for+logo+detection&amp;author=Y.+Bao&amp;author=H.+Li&amp;author=X.+Fan&amp;author=R.+Liu&amp;author=Q.+Jia&amp;pages=319-322&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B34-entropy-25-00174">34. <span class="element-citation">Velazquez D.A., Gonfaus J.M., Rodriguez P., Roca F.X., Ozawa S., Gonz&#x000e0;lez J. Logo Detection With No Priors. <span><span class="ref-journal">IEEE Access. </span>2021;<span class="ref-vol">9</span>:106998–107011. doi: 10.1109/ACCESS.2021.3101297.</span> [<a href="//doi.org/10.1109%2FACCESS.2021.3101297" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=IEEE+Access&amp;title=Logo+Detection+With+No+Priors&amp;author=D.A.+Velazquez&amp;author=J.M.+Gonfaus&amp;author=P.+Rodriguez&amp;author=F.X.+Roca&amp;author=S.+Ozawa&amp;volume=9&amp;publication_year=2021&amp;pages=106998-107011&amp;doi=10.1109/ACCESS.2021.3101297&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B35-entropy-25-00174">35. <span class="element-citation">Wang J., Zheng Y., Song J., Hou S. Cross-View Representation Learning for Multi-View Logo Classification with Information Bottleneck; Proceedings of the 29th ACM International Conference on Multimedia; Virtual. 20&#x02013;24 October 2021; pp. 4680–4688. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+29th+ACM+International+Conference+on+Multimedia&amp;title=Cross-View+Representation+Learning+for+Multi-View+Logo+Classification+with+Information+Bottleneck&amp;author=J.+Wang&amp;author=Y.+Zheng&amp;author=J.+Song&amp;author=S.+Hou&amp;pages=4680-4688&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B36-entropy-25-00174">36. <span class="element-citation">Liang T., Wang Y., Tang Z., Hu G., Ling H. Opanas: One-shot path aggregation network architecture search for object detection; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition; Nashville, TN, USA. 20&#x02013;25 June 2021; pp. 10195–10203. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+IEEE/CVF+Conference+on+Computer+Vision+and+Pattern+Recognition&amp;title=Opanas:+One-shot+path+aggregation+network+architecture+search+for+object+detection&amp;author=T.+Liang&amp;author=Y.+Wang&amp;author=Z.+Tang&amp;author=G.+Hu&amp;author=H.+Ling&amp;pages=10195-10203&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B37-entropy-25-00174">37. <span class="element-citation">Li D., Hu J., Wang C., Li X., She Q., Zhu L., Zhang T., Chen Q. Involution: Inverting the inherence of convolution for visual recognition; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition; Nashville, TN, USA. 20&#x02013;25 June 2021; pp. 12321–12330. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+IEEE/CVF+Conference+on+Computer+Vision+and+Pattern+Recognition&amp;title=Involution:+Inverting+the+inherence+of+convolution+for+visual+recognition&amp;author=D.+Li&amp;author=J.+Hu&amp;author=C.+Wang&amp;author=X.+Li&amp;author=Q.+She&amp;pages=12321-12330&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B38-entropy-25-00174">38. <span class="element-citation">Srinivas A., Lin T.Y., Parmar N., Shlens J., Abbeel P., Vaswani A. Bottleneck transformers for visual recognition; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition; Nashville, TN, USA. 20&#x02013;25 June 2021; pp. 16519–16529. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+IEEE/CVF+Conference+on+Computer+Vision+and+Pattern+Recognition&amp;title=Bottleneck+transformers+for+visual+recognition&amp;author=A.+Srinivas&amp;author=T.Y.+Lin&amp;author=N.+Parmar&amp;author=J.+Shlens&amp;author=P.+Abbeel&amp;pages=16519-16529&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B39-entropy-25-00174">39. <span class="element-citation">Zhu X., Hu H., Lin S., Dai J. Deformable convnets v2: More deformable, better results; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition; Long Beach, CA, USA. 15&#x02013;20 June 2019; pp. 9308–9316. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+IEEE/CVF+Conference+on+Computer+Vision+and+Pattern+Recognition&amp;title=Deformable+convnets+v2:+More+deformable,+better+results&amp;author=X.+Zhu&amp;author=H.+Hu&amp;author=S.+Lin&amp;author=J.+Dai&amp;pages=9308-9316&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B40-entropy-25-00174">40. <span class="element-citation">Su H., Zhu X., Gong S. Open logo detection challenge. <span><span class="ref-journal">arXiv. </span>2018</span>1807.01964 <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=arXiv&amp;title=Open+logo+detection+challenge&amp;author=H.+Su&amp;author=X.+Zhu&amp;author=S.+Gong&amp;publication_year=2018&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B41-entropy-25-00174">41. <span class="element-citation">Romberg S., Pueyo L.G., Lienhart R., Van Zwol R. Scalable logo recognition in real-world images; Proceedings of the 1st ACM International Conference on Multimedia Retrieval; Trento, Italy. 18&#x02013;20 April 2011; pp. 1–8. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+1st+ACM+International+Conference+on+Multimedia+Retrieval&amp;title=Scalable+logo+recognition+in+real-world+images&amp;author=S.+Romberg&amp;author=L.G.+Pueyo&amp;author=R.+Lienhart&amp;author=R.+Van+Zwol&amp;pages=1-8&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B42-entropy-25-00174">42. <span class="element-citation">Chen K., Wang J., Pang J., Cao Y., Xiong Y., Li X., Sun S., Feng W., Liu Z., Xu J., et al. MMDetection: Open mmlab detection toolbox and benchmark. <span><span class="ref-journal">arXiv. </span>2019</span>1906.07155 <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=arXiv&amp;title=MMDetection:+Open+mmlab+detection+toolbox+and+benchmark&amp;author=K.+Chen&amp;author=J.+Wang&amp;author=J.+Pang&amp;author=Y.+Cao&amp;author=Y.+Xiong&amp;publication_year=2019&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B43-entropy-25-00174">43. <span class="element-citation">Zhang H., Chang H., Ma B., Wang N., Chen X. Dynamic R-CNN: Towards high quality object detection via dynamic training; Proceedings of the European Conference on Computer Vision; Glasgow, UK. 23&#x02013;28 August 2020; Berlin/Heidelberg, Germany: Springer; 2020. pp. 260–275. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+European+Conference+on+Computer+Vision&amp;title=Dynamic+R-CNN:+Towards+high+quality+object+detection+via+dynamic+training&amp;author=H.+Zhang&amp;author=H.+Chang&amp;author=B.+Ma&amp;author=N.+Wang&amp;author=X.+Chen&amp;publication_year=2020&amp;pages=260-275&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B44-entropy-25-00174">44. <span class="element-citation">Everingham M., Van Gool L., Williams C.K., Winn J., Zisserman A. The pascal visual object classes (voc) challenge. <span><span class="ref-journal">Int. J. Comput. Vis. </span>2010;<span class="ref-vol">88</span>:303–338. doi: 10.1007/s11263-009-0275-4.</span> [<a href="//doi.org/10.1007%2Fs11263-009-0275-4" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Int.+J.+Comput.+Vis.&amp;title=The+pascal+visual+object+classes+(voc)+challenge&amp;author=M.+Everingham&amp;author=L.+Van+Gool&amp;author=C.K.+Williams&amp;author=J.+Winn&amp;author=A.+Zisserman&amp;volume=88&amp;publication_year=2010&amp;pages=303-338&amp;doi=10.1007/s11263-009-0275-4&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B45-entropy-25-00174">45. <span class="element-citation">Zhu C., He Y., Savvides M. Feature selective anchor-free module for single-shot object detection; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition; Long Beach, CA, USA. 15&#x02013;20 June 2019; pp. 840–849. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+IEEE/CVF+Conference+on+Computer+Vision+and+Pattern+Recognition&amp;title=Feature+selective+anchor-free+module+for+single-shot+object+detection&amp;author=C.+Zhu&amp;author=Y.+He&amp;author=M.+Savvides&amp;pages=840-849&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B46-entropy-25-00174">46. <span class="element-citation">Zhang S., Chi C., Yao Y., Lei Z., Li S.Z. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition; Seattle, WA, USA. 13&#x02013;19 June 2020; pp. 9759–9768. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+IEEE/CVF+Conference+on+Computer+Vision+and+Pattern+Recognition&amp;title=Bridging+the+gap+between+anchor-based+and+anchor-free+detection+via+adaptive+training+sample+selection&amp;author=S.+Zhang&amp;author=C.+Chi&amp;author=Y.+Yao&amp;author=Z.+Lei&amp;author=S.Z.+Li&amp;pages=9759-9768&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B47-entropy-25-00174">47. <span class="element-citation">Li X., Wang W., Wu L., Chen S., Hu X., Li J., Tang J., Yang J. Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection. <span><span class="ref-journal">Adv. Neural Inf. Process. Syst. </span>2020;<span class="ref-vol">33</span>:21002–21012.</span> <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Adv.+Neural+Inf.+Process.+Syst.&amp;title=Generalized+focal+loss:+Learning+qualified+and+distributed+bounding+boxes+for+dense+object+detection&amp;author=X.+Li&amp;author=W.+Wang&amp;author=L.+Wu&amp;author=S.+Chen&amp;author=X.+Hu&amp;volume=33&amp;publication_year=2020&amp;pages=21002-21012&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B48-entropy-25-00174">48. <span class="element-citation">Bodla N., Singh B., Chellappa R., Davis L.S. Soft-NMS&#x02013;improving object detection with one line of code; Proceedings of the IEEE International Conference on Computer Vision; Venice, Italy. 22&#x02013;29 October 2017; pp. 5561–5569. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+IEEE+International+Conference+on+Computer+Vision&amp;title=Soft-NMS&#x02013;improving+object+detection+with+one+line+of+code&amp;author=N.+Bodla&amp;author=B.+Singh&amp;author=R.+Chellappa&amp;author=L.S.+Davis&amp;pages=5561-5569&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B49-entropy-25-00174">49. <span class="element-citation">Rezatofighi H., Tsoi N., Gwak J., Sadeghian A., Reid I., Savarese S. Generalized intersection over union: A metric and a loss for bounding box regression; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition; Long Beach, CA, USA. 15&#x02013;20 June 2019; pp. 658–666. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+IEEE/CVF+Conference+on+Computer+Vision+and+Pattern+Recognition&amp;title=Generalized+intersection+over+union:+A+metric+and+a+loss+for+bounding+box+regression&amp;author=H.+Rezatofighi&amp;author=N.+Tsoi&amp;author=J.+Gwak&amp;author=A.+Sadeghian&amp;author=I.+Reid&amp;pages=658-666&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B50-entropy-25-00174">50. <span class="element-citation">Pang J., Chen K., Shi J., Feng H., Ouyang W., Lin D. Libra r-cnn: Towards balanced learning for object detection; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition; Long Beach, CA, USA. 15&#x02013;20 June 2019; pp. 821–830. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+IEEE/CVF+Conference+on+Computer+Vision+and+Pattern+Recognition&amp;title=Libra+r-cnn:+Towards+balanced+learning+for+object+detection&amp;author=J.+Pang&amp;author=K.+Chen&amp;author=J.+Shi&amp;author=H.+Feng&amp;author=W.+Ouyang&amp;pages=821-830&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B51-entropy-25-00174">51. <span class="element-citation">Zheng Z., Wang P., Liu W., Li J., Ye R., Ren D. Distance-IoU loss: Faster and better learning for bounding box regression; Proceedings of the AAAI Conference on Artificial Intelligence; New York, NY, USA. 7&#x02013;12 February 2020; pp. 12993–13000. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+AAAI+Conference+on+Artificial+Intelligence&amp;title=Distance-IoU+loss:+Faster+and+better+learning+for+bounding+box+regression&amp;author=Z.+Zheng&amp;author=P.+Wang&amp;author=W.+Liu&amp;author=J.+Li&amp;author=R.+Ye&amp;volume=Volume+34&amp;pages=12993-13000&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B52-entropy-25-00174">52. <span class="element-citation">Wang J., Zhang W., Cao Y., Chen K., Pang J., Gong T., Shi J., Loy C.C., Lin D. Side-aware boundary localization for more precise object detection; Proceedings of the European Conference on Computer Vision; Glasgow, UK. 23&#x02013;28 August 2020; Berlin/Heidelberg, Germany: Springer; 2020. pp. 403–419. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+European+Conference+on+Computer+Vision&amp;title=Side-aware+boundary+localization+for+more+precise+object+detection&amp;author=J.+Wang&amp;author=W.+Zhang&amp;author=Y.+Cao&amp;author=K.+Chen&amp;author=J.+Pang&amp;publication_year=2020&amp;pages=403-419&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B53-entropy-25-00174">53. <span class="element-citation">Sun P., Zhang R., Jiang Y., Kong T., Xu C., Zhan W., Tomizuka M., Li L., Yuan Z., Wang C., et al. Sparse r-cnn: End-to-end object detection with learnable proposals; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition; Nashville, TN, USA. 20&#x02013;25 June 2021; pp. 14454–14463. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+IEEE/CVF+Conference+on+Computer+Vision+and+Pattern+Recognition&amp;title=Sparse+r-cnn:+End-to-end+object+detection+with+learnable+proposals&amp;author=P.+Sun&amp;author=R.+Zhang&amp;author=Y.+Jiang&amp;author=T.+Kong&amp;author=C.+Xu&amp;pages=14454-14463&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B54-entropy-25-00174">54. <span class="element-citation">Kong T., Sun F., Liu H., Jiang Y., Li L., Shi J. Foveabox: Beyound anchor-based object detection. <span><span class="ref-journal">IEEE Trans. Image Process. </span>2020;<span class="ref-vol">29</span>:7389–7398. doi: 10.1109/TIP.2020.3002345.</span> [<a href="//doi.org/10.1109%2FTIP.2020.3002345" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CCrosslink%7CDOI">CrossRef</a>] <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=IEEE+Trans.+Image+Process.&amp;title=Foveabox:+Beyound+anchor-based+object+detection&amp;author=T.+Kong&amp;author=F.+Sun&amp;author=H.+Liu&amp;author=Y.+Jiang&amp;author=L.+Li&amp;volume=29&amp;publication_year=2020&amp;pages=7389-7398&amp;doi=10.1109/TIP.2020.3002345&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B55-entropy-25-00174">55. <span class="element-citation">Wu Y., Chen Y., Yuan L., Liu Z., Wang L., Li H., Fu Y. Rethinking classification and localization for object detection; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition; Seattle, WA, USA. 13&#x02013;19 June 2020; pp. 10186–10195. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+IEEE/CVF+Conference+on+Computer+Vision+and+Pattern+Recognition&amp;title=Rethinking+classification+and+localization+for+object+detection&amp;author=Y.+Wu&amp;author=Y.+Chen&amp;author=L.+Yuan&amp;author=Z.+Liu&amp;author=L.+Wang&amp;pages=10186-10195&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B56-entropy-25-00174">56. <span class="element-citation">Iandola F.N., Shen A., Gao P., Keutzer K. Deeplogo: Hitting logo recognition with the deep neural network hammer. <span><span class="ref-journal">arXiv. </span>2015</span>1510.02131 <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=arXiv&amp;title=Deeplogo:+Hitting+logo+recognition+with+the+deep+neural+network+hammer&amp;author=F.N.+Iandola&amp;author=A.+Shen&amp;author=P.+Gao&amp;author=K.+Keutzer&amp;publication_year=2015&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div><div class="ref-cit-blk half_rhythm" id="B57-entropy-25-00174">57. <span class="element-citation">Oliveira G., Fraz&#x000e3;o X., Pimentel A., Ribeiro B. Automatic graphic logo detection via fast region-based convolutional networks; Proceedings of the 2016 International Joint Conference on Neural Networks; Vancouver, BC, Canada. 24&#x02013;29 July 2016; pp. 985–991. <span class="nowrap">[<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings+of+the+2016+International+Joint+Conference+on+Neural+Networks&amp;title=Automatic+graphic+logo+detection+via+fast+region-based+convolutional+networks&amp;author=G.+Oliveira&amp;author=X.+Fraz&#x000e3;o&amp;author=A.+Pimentel&amp;author=B.+Ribeiro&amp;pages=985-991&amp;" target="_blank" rel="noopener noreferrer" ref="reftype=other&amp;article-id=9857861&amp;issue-id=427036&amp;journal-id=3926&amp;FROM=Article%7CCitationRef&amp;TO=Content%20Provider%7CLink%7CGoogle%20Scholar">Google Scholar</a>]</span></span></div></div></div></div><!--post-content--><div class="courtesy-note whole_rhythm small"><hr /><div class="half_rhythm">Articles from <span class="acknowledgment-journal-title">Entropy</span> are provided here courtesy of <strong>Multidisciplinary Digital Publishing Institute  (MDPI)</strong></div><hr /></div><div id="body-link-poppers"><span></span></div></div>
            
        </section>
    </article>
    <aside class="usa-width-one-fourth usa-layout-docs-sidenav pmc-sidebar">
         
  

<div class="scroller">

    
        <section>
                <h6>Other Formats</h6>
                <ul class="pmc-sidebar__formats">
                  <li class="pubreader-link other_item"><a href="/pmc/articles/PMC9857861/?report=reader" class="sidefm-pmclink">PubReader</a></li>
<li class="pdf-link other_item"><a href="/pmc/articles/PMC9857861/pdf/entropy-25-00174.pdf" class="int-view">PDF (3.0M)</a></li>
                </ul>
        </section>
    
    <section>
        <h6>Actions</h6>
        <ul class="pmc-sidebar__actions">
            <li>
                <button role="button" class="citation-button citation-dialog-trigger ctxp"
                        aria-label="Open dialog with citation text in different styles" data-ga-category="save_share" data-ga-action="cite" data-ga-label="open"
                        data-all-citations-url="/pmc/resources/citations/9857861/"
                        data-citation-style="nlm"
                        data-download-format-link="/pmc/resources/citations/9857861/export/"
                >
                    <span class="button-label">Cite</span>
                </button>
            </li>
            <li>
                
                    

<div class="collections-button-container" data-article-id="9857861" data-article-db="pmc">
  <button class="collections-button collections-dialog-trigger"
          aria-label="Save article in MyNCBI collections."
          data-ga-category="collections_button"
          data-ga-action="click"
          data-ga-label="collections_button"
          data-collections-open-dialog-enabled="false"
          data-collections-open-dialog-url="https://www.ncbi.nlm.nih.gov/account?back_url=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC9857861%2F%23open-collections-dialog"
          data-in-collections="false">
      <span class="button-label">Collections</span>
  </button>
  <div class="overlay" role="dialog">
  <div id="collections-action-dialog"
       class="dialog collections-dialog"
       aria-hidden="true">
    <div class="title">Add to Collections</div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/pmc/list-existing-collections/"
      data-add-to-existing-collection-url="/pmc/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/pmc/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

  <input type="hidden" name="csrfmiddlewaretoken" value="Q8UnHfwsxk4jqadNUceLBKWuZuqM4zGN7iTgTJMbt0CIdNYca6Gg2m3tb87thdUJ">

  

  <div class="choice-group" role="radiogroup">
    <ul class="radio-group-items">
      <li>
        <input type="radio"
               id="collections-action-dialog-new-aside "
               class="collections-new"
               name="collections"
               value="new"
               data-ga-category="collections_button"
               data-ga-action="click"
               data-ga-label="collections_radio_new">
        <label for="collections-action-dialog-new-aside ">Create a new collection</label>
      </li>
      <li>
        <input type="radio"
               id="collections-action-dialog-existing-aside "
               class="collections-existing"
               name="collections"
               value="existing"
               checked="true"
               data-ga-category="collections_button"
               data-ga-action="click"
               data-ga-label="collections_radio_existing">
        <label for="collections-action-dialog-existing-aside ">Add to an existing collection</label>
      </li>
    </ul>
  </div>

  <div class="controls-wrapper">
    <div class="action-panel-control-wrap new-collections-controls">
      <label for="collections-action-dialog-add-to-new" class="action-panel-label required-field-asterisk">
        Name your collection:
      </label>
      <input
        type="text"
        name="add-to-new-collection"
        id="collections-action-dialog-add-to-new"
        class="collections-action-add-to-new"
        pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
        maxlength=""
        data-ga-category="collections_button"
        data-ga-action="create_collection"
        data-ga-label="non_favorties_collection">
      <div class="collections-new-name-too-long usa-input-error-message selection-validation-message">
        Name must be less than  characters
      </div>
    </div>
    <div class="action-panel-control-wrap existing-collections-controls">
      <label for="collections-action-dialog-add-to-existing" class="action-panel-label">
        Choose a collection:
      </label>
      <select id="collections-action-dialog-add-to-existing"
              class="action-panel-selector collections-action-add-to-existing"
              data-ga-category="collections_button"
              data-ga-action="select_collection"
              data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
      </select>
      <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
        Unable to load your collection due to an error<br>
        <a href="#">Please try again</a>
      </div>
    </div>
  </div>

  <div class="action-panel-actions">
    <button class="action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
      Add
    </button>
    <button class="action-panel-cancel"
            aria-label="Close 'Add to Collections' panel"
            ref="linksrc=close_collections_panel"
            aria-controls="collections-action-panel"
            aria-expanded="false"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="cancel">
      Cancel
    </button>
  </div>
</form>
    </div>
  </div>
</div>
</div>
                
            </li>

        </ul>
    </section>
    
        <section class="social-sharing">
            <h6>Share</h6>
            <ul class="pmc-sidebar__share">
                <li><a class="fa-stack fa-lg" target="_blank" rel="noopener noreferrer" role="button" href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC9857861%2F&amp;text=Long-Range%20Dependence%20Involutional%20Network%20for%20Logo%20Detection" alt="Share on Twitter"><i class="fa fa-twitter fa-stack-1x">&#160;</i></a></li> 
<li><a class="fa-stack fa-lg" target="_blank" rel="noopener noreferrer" role="button" href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC9857861%2F" alt="Share on Facebook"><i class="fa fa-facebook fa-stack-1x">&#160;</i></a></li>
                <li>
                    <div class="share-permalink">
                        <button class="trigger"  alt="Show article permalink" aria-expanded="false" aria-haspopup="true">
                            <i class="fa-stack fa-lg" >
                                <i class="fa fa-link fa-stack-1x">&nbsp;</i>
                            </i>
                        </button>
                        <div class="dropdown dropdown-container" hidden>
                              <div class="title">
                                Permalink
                              </div>
                              <div class="content">
                                  <input type="text" class="permalink-text" value="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9857861/" aria-label="Article permalink"><button class="permalink-copy-button usa-button-primary" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                                      <span class="button-title">Copy</span>
                                  </button>
                              </div>
                        </div>
                    </div>
                </li>
            </ul>
        </section>
    
    <section>
        <h6>RESOURCES</h6>
        <ul class="pmc-sidebar__resources">
        
            <li>
                <div class="usa-accordion">
                    <button
                            data-ga-category="resources_accordion"
                            data-ga-action="open_similar_articles"
                            data-ga-label="/pmc/articles/PMC9857861/"
                            class="usa-accordion-button"
                            aria-controls="similar-articles-accordion-aside"
                            aria-expanded="false"
                            data-action-open="open_similar_articles"
                            data-action-close="close_similar_articles"
                    >
                        Similar articles
                    </button>
                    <div
                            
                                data-source-url="/pmc/resources/similar-article-links/36673315/"
                            

                         class="usa-accordion-content pmc-sidebar__resources--citations" id="similar-articles-accordion-aside" aria-hidden="true">
                        
                    </div>
                </div>
            </li>
            <li>
                <div class="usa-accordion">
                    <button
                            data-ga-category="resources_accordion"
                            data-ga-action="open_cited_by"
                            data-ga-label="/pmc/articles/PMC9857861/"
                            class="usa-accordion-button"
                            aria-controls="cited-by-accordion-aside"
                            aria-expanded="false"
                            data-action-open="open_cited_by"
                            data-action-close="close_cited_by"
                    >
                        Cited by other articles
                    </button>
                    <div
                            
                                data-source-url="/pmc/resources/cited-by-links/36673315/"
                            
                            class="usa-accordion-content pmc-sidebar__resources--citations"
                            id="cited-by-accordion-aside"
                            aria-hidden="true"
                    >
                        
                    </div>
                </div>
            </li>
            <li>
                <div class="usa-accordion">
                    <button
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/pmc/articles/PMC9857861/"
                            class="usa-accordion-button"
                            aria-controls="links-accordion-aside"
                            aria-expanded="false"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                    >
                        Links to NCBI Databases
                    </button>
                    <div data-source-url="/pmc/resources/db-links/9857861/" class="usa-accordion-content" id="links-accordion-aside" aria-hidden="true"></div>
                </div>
            </li>

            
        
        </ul>
    </section>

 </div>

    </aside>
</main>

    <div class="overlay" role="dialog" aria-label="Citation Dialog">
  <div class="dialog citation-dialog">
    <button class="close-overlay" tabindex="1">[x]</button>
    <div class="title">Cite</div>
    <div class="citation-text-block">
  <div class="citation-text"></div>
  <div class="citation-actions">
    <button
      class="copy-button dialog-focus"
      data-ga-category="save_share"
      data-ga-action="cite"
      data-ga-label="copy"
      tabindex="2">
      Copy
    </button>

      <a href="#"
              class="export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
          <span class="title">Download .nbib</span>
          <span class="title-mobile">.nbib</span>
      </a>


    

<div class="citation-style-selector-wrapper">
  <label class="selector-label">Format:</label>
  <select aria-label="Format" class="citation-style-selector" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
  </div>
</div>
  </div>
</div>

     <!-- ========== BEGIN FOOTER ========== -->
 <footer>
      <section class="icon-section">
        <div id="icon-section-header" class="icon-section_header">Follow NCBI</div>
        <div class="grid-container container">
          <div class="icon-section_container">
            <a class="footer-icon" id="footer_twitter" href="https://twitter.com/ncbi" aria-label="Twitter"><svg
                data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 300 300">
                <defs>
                  <style>
                    .cls-11 {
                      fill: #737373;
                    }
                  </style>
                </defs>
                <title>Twitter</title>
                <path class="cls-11"
                  d="M250.11,105.48c-7,3.14-13,3.25-19.27.14,8.12-4.86,8.49-8.27,11.43-17.46a78.8,78.8,0,0,1-25,9.55,39.35,39.35,0,0,0-67,35.85,111.6,111.6,0,0,1-81-41.08A39.37,39.37,0,0,0,81.47,145a39.08,39.08,0,0,1-17.8-4.92c0,.17,0,.33,0,.5a39.32,39.32,0,0,0,31.53,38.54,39.26,39.26,0,0,1-17.75.68,39.37,39.37,0,0,0,36.72,27.3A79.07,79.07,0,0,1,56,223.34,111.31,111.31,0,0,0,116.22,241c72.3,0,111.83-59.9,111.83-111.84,0-1.71,0-3.4-.1-5.09C235.62,118.54,244.84,113.37,250.11,105.48Z">
                </path>
              </svg></a>
            <a class="footer-icon" id="footer_facebook" href="https://www.facebook.com/ncbi.nlm" aria-label="Facebook"><svg
                data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 300 300">
                <title>Facebook</title>
                <path class="cls-11"
                  d="M210.5,115.12H171.74V97.82c0-8.14,5.39-10,9.19-10h27.14V52l-39.32-.12c-35.66,0-42.42,26.68-42.42,43.77v19.48H99.09v36.32h27.24v109h45.41v-109h35Z">
                </path>
              </svg></a>
            <a class="footer-icon" id="footer_linkedin"
              href="https://www.linkedin.com/company/ncbinlm"
              aria-label="LinkedIn"><svg data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 300 300">
                <title>LinkedIn</title>
                <path class="cls-11"
                  d="M101.64,243.37H57.79v-114h43.85Zm-22-131.54h-.26c-13.25,0-21.82-10.36-21.82-21.76,0-11.65,8.84-21.15,22.33-21.15S101.7,78.72,102,90.38C102,101.77,93.4,111.83,79.63,111.83Zm100.93,52.61A17.54,17.54,0,0,0,163,182v61.39H119.18s.51-105.23,0-114H163v13a54.33,54.33,0,0,1,34.54-12.66c26,0,44.39,18.8,44.39,55.29v58.35H198.1V182A17.54,17.54,0,0,0,180.56,164.44Z">
                </path>
              </svg></a>
            <a class="footer-icon" id="footer_github" href="https://github.com/ncbi" aria-label="GitHub"><svg
                data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 300 300">
                <defs>
                  <style>
                    .cls-11,
                    .cls-12 {
                      fill: #737373;
                    }

                    .cls-11 {
                      fill-rule: evenodd;
                    }
                  </style>
                </defs>
                <title>GitHub</title>
                <path class="cls-11"
                  d="M151.36,47.28a105.76,105.76,0,0,0-33.43,206.1c5.28,1,7.22-2.3,7.22-5.09,0-2.52-.09-10.85-.14-19.69-29.42,6.4-35.63-12.48-35.63-12.48-4.81-12.22-11.74-15.47-11.74-15.47-9.59-6.56.73-6.43.73-6.43,10.61.75,16.21,10.9,16.21,10.9,9.43,16.17,24.73,11.49,30.77,8.79,1-6.83,3.69-11.5,6.71-14.14C108.57,197.1,83.88,188,83.88,147.51a40.92,40.92,0,0,1,10.9-28.39c-1.1-2.66-4.72-13.42,1-28,0,0,8.88-2.84,29.09,10.84a100.26,100.26,0,0,1,53,0C198,88.3,206.9,91.14,206.9,91.14c5.76,14.56,2.14,25.32,1,28a40.87,40.87,0,0,1,10.89,28.39c0,40.62-24.74,49.56-48.29,52.18,3.79,3.28,7.17,9.71,7.17,19.58,0,14.15-.12,25.54-.12,29,0,2.82,1.9,6.11,7.26,5.07A105.76,105.76,0,0,0,151.36,47.28Z">
                </path>
                <path class="cls-12"
                  d="M85.66,199.12c-.23.52-1.06.68-1.81.32s-1.2-1.06-.95-1.59,1.06-.69,1.82-.33,1.21,1.07.94,1.6Zm-1.3-1">
                </path>
                <path class="cls-12"
                  d="M90,203.89c-.51.47-1.49.25-2.16-.49a1.61,1.61,0,0,1-.31-2.19c.52-.47,1.47-.25,2.17.49s.82,1.72.3,2.19Zm-1-1.08">
                </path>
                <path class="cls-12"
                  d="M94.12,210c-.65.46-1.71,0-2.37-.91s-.64-2.07,0-2.52,1.7,0,2.36.89.65,2.08,0,2.54Zm0,0"></path>
                <path class="cls-12"
                  d="M99.83,215.87c-.58.64-1.82.47-2.72-.41s-1.18-2.06-.6-2.7,1.83-.46,2.74.41,1.2,2.07.58,2.7Zm0,0">
                </path>
                <path class="cls-12"
                  d="M107.71,219.29c-.26.82-1.45,1.2-2.64.85s-2-1.34-1.74-2.17,1.44-1.23,2.65-.85,2,1.32,1.73,2.17Zm0,0">
                </path>
                <path class="cls-12"
                  d="M116.36,219.92c0,.87-1,1.59-2.24,1.61s-2.29-.68-2.3-1.54,1-1.59,2.26-1.61,2.28.67,2.28,1.54Zm0,0">
                </path>
                <path class="cls-12"
                  d="M124.42,218.55c.15.85-.73,1.72-2,1.95s-2.37-.3-2.52-1.14.73-1.75,2-2,2.37.29,2.53,1.16Zm0,0"></path>
              </svg></a>
            <a class="footer-icon" id="footer_blog" href="https://ncbiinsights.ncbi.nlm.nih.gov/" aria-label="Blog">
              <svg id="Layer_1" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 40"><defs><style>.cls-1{fill:#737373;}</style></defs><path class="cls-1" d="M14,30a4,4,0,1,1-4-4,4,4,0,0,1,4,4Zm11,3A19,19,0,0,0,7.05,15a1,1,0,0,0-1,1v3a1,1,0,0,0,.93,1A14,14,0,0,1,20,33.07,1,1,0,0,0,21,34h3a1,1,0,0,0,1-1Zm9,0A28,28,0,0,0,7,6,1,1,0,0,0,6,7v3a1,1,0,0,0,1,1A23,23,0,0,1,29,33a1,1,0,0,0,1,1h3A1,1,0,0,0,34,33Z"/></svg>
            </a>
          </div>
        </div>
      </section>

      <section class="container-fluid bg-primary">
        <div class="container pt-5">
          <div class="row mt-3">
            <div class="col-lg-3 col-12">
              <p><a class="text-white" href="https://www.nlm.nih.gov/socialmedia/index.html">Connect with NLM</a></p>
              <ul class="list-inline social_media">
                <li class="list-inline-item"><a href="https://twitter.com/NLM_NIH" aria-label="Twitter"
                    target="_blank" rel="noopener noreferrer"><svg version="1.1" xmlns="http://www.w3.org/2000/svg"
                      xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 249 249"
                      style="enable-background:new 0 0 249 249;" xml:space="preserve">
                      <style type="text/css">
                        .st20 {
                          fill: #FFFFFF;
                        }

                        .st30 {
                          fill: none;
                          stroke: #FFFFFF;
                          stroke-width: 8;
                          stroke-miterlimit: 10;
                        }
                      </style>
                      <title>SM-Twitter</title>
                      <g>
                        <g>
                          <g>
                            <path class="st20" d="M192.9,88.1c-5,2.2-9.2,2.3-13.6,0.1c5.7-3.4,6-5.8,8.1-12.3c-5.4,3.2-11.4,5.5-17.6,6.7
                                                c-10.5-11.2-28.1-11.7-39.2-1.2c-7.2,6.8-10.2,16.9-8,26.5c-22.3-1.1-43.1-11.7-57.2-29C58,91.6,61.8,107.9,74,116
                                                c-4.4-0.1-8.7-1.3-12.6-3.4c0,0.1,0,0.2,0,0.4c0,13.2,9.3,24.6,22.3,27.2c-4.1,1.1-8.4,1.3-12.5,0.5c3.6,11.3,14,19,25.9,19.3
                                                c-11.6,9.1-26.4,13.2-41.1,11.5c12.7,8.1,27.4,12.5,42.5,12.5c51,0,78.9-42.2,78.9-78.9c0-1.2,0-2.4-0.1-3.6
                                                C182.7,97.4,189.2,93.7,192.9,88.1z"></path>
                          </g>
                        </g>
                        <circle class="st30" cx="124.4" cy="128.8" r="108.2"></circle>
                      </g>
                    </svg></a></li>
                <li class="list-inline-item"><a href="https://www.facebook.com/nationallibraryofmedicine"
                    aria-label="Facebook" rel="noopener noreferrer" target="_blank">
                    <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px"
                      y="0px" viewBox="0 0 249 249" style="enable-background:new 0 0 249 249;" xml:space="preserve">
                      <style type="text/css">
                        .st10 {
                          fill: #FFFFFF;
                        }

                        .st110 {
                          fill: none;
                          stroke: #FFFFFF;
                          stroke-width: 8;
                          stroke-miterlimit: 10;
                        }
                      </style>
                      <title>SM-Facebook</title>
                      <g>
                        <g>
                          <path class="st10" d="M159,99.1h-24V88.4c0-5,3.3-6.2,5.7-6.2h16.8V60l-24.4-0.1c-22.1,0-26.2,16.5-26.2,27.1v12.1H90v22.5h16.9
                                                      v67.5H135v-67.5h21.7L159,99.1z"></path>
                        </g>
                      </g>
                      <circle class="st110" cx="123.6" cy="123.2" r="108.2"></circle>
                    </svg>
                  </a></li>
                <li class="list-inline-item"><a href="https://www.youtube.com/user/NLMNIH" aria-label="Youtube"
                    target="_blank" rel="noopener noreferrer"><svg version="1.1" xmlns="http://www.w3.org/2000/svg"
                      xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 249 249"
                      style="enable-background:new 0 0 249 249;" xml:space="preserve">
                      <title>SM-Youtube</title>
                      <style type="text/css">
                        .st4 {
                          fill: none;
                          stroke: #FFFFFF;
                          stroke-width: 8;
                          stroke-miterlimit: 10;
                        }

                        .st5 {
                          fill: #FFFFFF;
                        }
                      </style>
                      <circle class="st4" cx="124.2" cy="123.4" r="108.2"></circle>
                      <g transform="translate(0,-952.36218)">
                        <path class="st5"
                          d="M88.4,1037.4c-10.4,0-18.7,8.3-18.7,18.7v40.1c0,10.4,8.3,18.7,18.7,18.7h72.1c10.4,0,18.7-8.3,18.7-18.7
                                            v-40.1c0-10.4-8.3-18.7-18.7-18.7H88.4z M115.2,1058.8l29.4,17.4l-29.4,17.4V1058.8z"></path>
                      </g>
                    </svg></a></li>
              </ul>
            </div>
            <div class="col-lg-3 col-12">
              <p class="address_footer text-white">National Library of Medicine<br>
                <a href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/@38.9959508,-77.101021,17z/data=!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb:0x19156f88b27635b8!8m2!3d38.9959508!4d-77.0988323"
                  class="text-white" target="_blank" rel="noopener noreferrer">8600 Rockville Pike<br>
                  Bethesda, MD 20894</a></p>
            </div>
            <div class="col-lg-3 col-12 centered-lg">
              <p><a href="https://www.nlm.nih.gov/web_policies.html" class="text-white">Web Policies</a><br>
                <a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office"
                  class="text-white">FOIA</a><br>
                <a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="text-white" id="vdp">HHS Vulnerability Disclosure</a></p>
            </div>
            <div class="col-lg-3 col-12 centered-lg">
              <p><a class="supportLink text-white" href="https://support.nlm.nih.gov/">Help</a><br>
                <a href="https://www.nlm.nih.gov/accessibility.html" class="text-white">Accessibility</a><br>
                <a href="https://www.nlm.nih.gov/careers/careers.html" class="text-white">Careers</a></p>
            </div>
          </div>
          <div class="row">
            <div class="col-lg-12 centered-lg">
              <nav class="bottom-links">
                <ul class="mt-3">
                  <li>
                    <a class="text-white" href="//www.nlm.nih.gov/">NLM</a>
                  </li>
                  <li>
                    <a class="text-white"
                  href="https://www.nih.gov/">NIH</a>
                  </li>
                  <li>
                    <a class="text-white" href="https://www.hhs.gov/">HHS</a>
                  </li>
                  <li>
                    <a
                  class="text-white" href="https://www.usa.gov/">USA.gov</a>
                  </li>
                </ul>
              </nav>
            </div>
          </div>
        </div>
      </section>
    </footer>
 <!-- ========== END FOOTER ========== -->
  <!-- javascript to inject NWDS meta tags. Note: value of nwds_version is updated by "npm version" command -->
 
  <script type="text/javascript">
    var nwds_version = "1.1.9-2";

    var meta_nwds_ver = document.createElement('meta');
    meta_nwds_ver.name = 'ncbi_nwds_ver';
    meta_nwds_ver.content = nwds_version;
    document.getElementsByTagName('head')[0].appendChild(meta_nwds_ver);

    var meta_nwds = document.createElement('meta');
    meta_nwds.name = 'ncbi_nwds';
    meta_nwds.content = 'yes';
    document.getElementsByTagName('head')[0].appendChild(meta_nwds);

	var alertsUrl = "/core/alerts/alerts.js";
	if (typeof ncbiBaseUrl !== 'undefined') {
		alertsUrl = ncbiBaseUrl + alertsUrl;
	}
  </script>



  
    <!-- JavaScript -->
    <script src="/pmc/static/CACHE/js/output.0f72d6a64937.js"></script>
  
  
    <script src="https://code.jquery.com/jquery-3.5.0.min.js"
          integrity="sha256-xNzN2a4ltkB44Mc/Jz3pT4iU1cmeR0FkXs4pru/JxaQ="
          crossorigin="anonymous">
    </script>
    <script>
        var fallbackJquery = "/pmc/static/base/js/jquery-3.5.0.min.js";
        window.jQuery || document.write("<script src=" + fallbackJquery + ">\x3C/script>")
    </script>
  

  <script src="/pmc/static/CACHE/js/output.a212a9fcf845.js"></script>
<script src="/pmc/static/CACHE/js/output.7999321d1aac.js"></script>
<script src="/pmc/static/CACHE/js/output.7ca436b2ea51.js"></script>
<script src="/pmc/static/CACHE/js/output.f8422046fbe0.js"></script>
<script src="/pmc/static/CACHE/js/output.ff40c7d85ff8.js"></script>
<script src="/pmc/static/CACHE/js/output.a6a84a0ad361.js"></script>

<script type="text/javascript" src="/pmc/static/bundles/base/base.54110350c77632754ed7.js" ></script>

    <script type="text/javascript">
        if(typeof jQuery !=='undefined') {
            jQuery.migrateMute = true;
        }
    </script>
    <script type="text/javascript" src="https://code.jquery.com/jquery-migrate-1.4.1.js"></script>
    <script type="text/javascript" src="/core/jig/1.15.2/js/jig.nojquery.min.js">//</script><script type="text/javascript" src="/corehtml/pmc/js/common.min.js?_=3.18">//</script><script type="text/javascript" src="/corehtml/pmc/js/NcbiTagServer.min.js?_=3.18">//</script><script type="text/javascript" src="/corehtml/pmc/js/crb.min.js?_=3.18">//</script><script type="text/javascript" src="/corehtml/pmc/js/jactions.min.js?_=3.18">//</script><meta name="citationexporter" content="backend:'https://api.ncbi.nlm.nih.gov/lit/ctxp/v1/pmc/'" /><script type="text/javascript" src="https://www.ncbi.nlm.nih.gov/corehtml/pmc/ctxp/jquery.citationexporter.min.js">//</script><link rel="stylesheet" href="https://www.ncbi.nlm.nih.gov/corehtml/pmc/ctxp/citationexporter.css" type="text/css" /><script type="text/javascript" src="/core/mathjax/2.7.9/MathJax.js?config=/corehtml/pmc/js/mathjax-config-classic.3.4.js"></script><script type="text/javascript">window.name="mainwindow";</script>

    <script type="text/javascript">var exports = {};</script>
    <script src="/pmc/static/CACHE/js/output.340a3b9cce7f.js"></script>
<script src="/pmc/static/CACHE/js/output.586993420cfd.js"></script>
    <script type="text/javascript" src="/pmc/static/bundles/article/article.bf4d0b9d90eb88f67344.js" ></script>
    <script type="text/javascript">
        window.ncbi.pmc.articlePage.init({ pageURL: '/pmc/articles/PMC9857861/', citeCookieName: 'pmc-cf'});
    </script>


  
  
  <script  type="text/javascript" src="https://www.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


  
      
  

</body>
</html>
